{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx76qjaGKwIS"
   },
   "source": [
    "# æ—¢å­˜è³‡æ–™ã‹ã‚‰ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ä½œæˆã™ã‚‹(Retrieval Augumented Generation:RAG)\n",
    "\n",
    "- å‚è€ƒ\n",
    "  - https://colab.research.google.com/github/nyanta012/demo/blob/main/sentence_retrieval.ipynb\n",
    "  - https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
    "  - https://note.com/mahlab/n/nb6677d0fc7c2\n",
    "- ChromadbãŒã™ã”ã„\n",
    "  - https://www.trychroma.com/\n",
    "  - https://github.com/chroma-core/chroma\n",
    "- ChatGPTã®Surveyè«–æ–‡\n",
    "  - https://arxiv.org/pdf/2304.01852.pdf\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6477,
     "status": "ok",
     "timestamp": 1744862988591,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "fC7q6QnDPIW2"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA, RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "import openai\n",
    "import os\n",
    "# langchain-openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1744862993307,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "SZEvBNCdPxmp"
   },
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 18069,
     "status": "ok",
     "timestamp": 1744863035248,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "1RVC0SOUL1Dp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text, metadata):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata\n",
    "\n",
    "dir_path = \"loadtext/\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    metadata = {'filename': filename}\n",
    "\n",
    "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    if filename.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "            documents.append(Document(text, metadata))\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    elif filename.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            documents.append(Document(text, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3637,
     "status": "ok",
     "timestamp": 1744863063789,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "oamHbgKbRBFI"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14384,
     "status": "ok",
     "timestamp": 1744863080365,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "duuXVCZlwPst",
    "outputId": "8af590ce-4923-438d-e7cd-179db285c54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ç·šå½¢ä»£æ•°ã«ãŠã„ã¦ã€ãƒ™ã‚¯ãƒˆãƒ«ã¨ã¯ä½•ã‚’æŒ‡ã—ã¾ã™ã‹ï¼Ÿ\n",
      "2. ç·šå½¢å†™åƒã¨ã¯ä½•ã§ã™ã‹ï¼Ÿå…·ä½“çš„ãªä¾‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
      "3. ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã™ã‚‹ç·šå½¢å¤‰æ›ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
      "4. 'ğ‘“(ğ’™)'ã¨ 'ğ‘”(ğ‘“ğ’™)'ã®é•ã„ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
      "5. ç·šå½¢ä»£æ•°ã®ä¸­ã§ã€å†™åƒã®åˆæˆã¨ã¯ä½•ã‚’æŒ‡ã—ã¾ã™ã‹ï¼Ÿ\n",
      "6. 'ğ¾ğ‘’ğ‘Ÿğ‘“'ã¨ã¯ä½•ã‚’æ„å‘³ã—ã¾ã™ã‹ï¼Ÿ\n",
      "7. 'ğ‘‘ğ‘–ğ‘šğ‘‰=dimğ¾ğ‘’ğ‘Ÿğ‘“+dim(ğ¼ğ‘šğ‘“)'ã¨ã„ã†ç­‰å¼ã®æ„å‘³ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
      "8. ç·šå½¢ä»£æ•°ã«ãŠã‘ã‚‹é€£ç¶šã—ãŸä¸€æ¬¡æ–¹ç¨‹å¼ã®è§£ãæ–¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
      "9. ç·šå½¢ä»£æ•°ã‚’æ´»ç”¨ã—ãŸãƒªã‚¢ãƒ«ãƒ¯ãƒ¼ãƒ«ãƒ‰ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¯ä½•ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ\n",
      "10. Pythonã§ç·šå½¢ä»£æ•°ã®å•é¡Œã‚’è§£ãéš›ã®ä¸€èˆ¬çš„ãªæ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è³ªå•ã‚’ä½œæˆã™ã‚‹ãŸã‚ã« invoke ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨\n",
    "questions = qa.invoke(\"ã“ã®æ–‡ç« ã®ä¸­ã§é‡è¦ã©æ€ã†ç·šå½¢ä»£æ•°ã«é–¢ã™ã‚‹å†…å®¹ã«ã¤ã„ã¦ã‚’è€ƒãˆã‚‹è³ªå•ã‚’10å€‹è€ƒãˆã¦ãã ã•ã„\")\n",
    "\n",
    "# çµæœã®è¡¨ç¤º\n",
    "print(questions['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4227,
     "status": "ok",
     "timestamp": 1744863124931,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "WuiXiS92vWju",
    "outputId": "9c8efc9e-76ec-450a-f479-ef2af65b4efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§ã¯ç–‘ä¼¼é€†è¡Œåˆ—ã«ã¤ã„ã¦ã®å…·ä½“çš„ãªçŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã›ã‚“ã€‚æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ããªã„ãŸã‚ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šã®ç†ç”±ã‹ã‚‰è©³ç´°ãªèª¬æ˜ã¯æä¾›ã§ãã¾ã›ã‚“ã€‚ã‚ãªãŸã®è³ªå•ã«å¯¾ã™ã‚‹ç–‘ä¼¼é€†è¡Œåˆ—ã®è©³ç´°ãªèª¬æ˜ã‚’å¾—ã‚‹ãŸã‚ã«ã¯ã€ä¿¡é ¼ã§ãã‚‹è³‡æ–™ã‚„å‚è€ƒæ›¸ç±ã‚’èª¿ã¹ã¦ã¿ã¦ãã ã•ã„ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è³ªå•ã‚’ä½œæˆã™ã‚‹ãŸã‚ã« invoke ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨\n",
    "questions = qa.invoke(\"ç–‘ä¼¼é€†è¡Œåˆ—ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿæ¤œç´¢ã§ããªã‘ã‚Œã°ã‚ã‹ã‚‰ãªã„ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# çµæœã®è¡¨ç¤º\n",
    "print(questions['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 15160,
     "status": "ok",
     "timestamp": 1744863145195,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05483787995557177837"
     },
     "user_tz": -540
    },
    "id": "wh0jw-neN2kc",
    "outputId": "ba4b2d47-dad7-45da-afb8-fbb12ce02f64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Takayama Ryuji\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://281b2fb5562b54719d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://281b2fb5562b54719d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import Rec1ursiveCharacterTextSplitter\n",
    "from langchain.chains import VectorDBQA, RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import gradio as gr\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "dir_path = 'loadtext/'\n",
    "\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    metadata = {'filename': filename}\n",
    "\n",
    "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    if filename.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "            documents.append(Document(text, metadata))\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    elif filename.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            documents.append(Document(text, metadata))\n",
    "\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²ã¨ãƒ™ã‚¯ãƒˆãƒ«DBã®ä½œæˆ\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# QAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())\n",
    "\n",
    "# ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…\n",
    "def chat_with_ai(input_text):\n",
    "    response = qa.invoke(input_text+'æ¤œç´¢ã§ããŸç¯„å›²ã§ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢ã§ããªã„ã‚‚ã®ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚')\n",
    "    return response['result']\n",
    "\n",
    "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®è¨­å®š\n",
    "iface = gr.Interface(\n",
    "    fn=chat_with_ai,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"AI Chat with Gradio\",\n",
    "    description=\"OpenAIã‚’ä½¿ã£ãŸãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\"\n",
    ")\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 11785,
     "status": "ok",
     "timestamp": 1721112692893,
     "user": {
      "displayName": "ä¸­è¥¿å´‡æ–‡",
      "userId": "05356289898840747459"
     },
     "user_tz": -540
    },
    "id": "1DBYaPehIXn4",
    "outputId": "41675b6c-0c96-4446-e22f-25ab82f6b930"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nq/n3nkdz2558g2zjbwzth6xy700000gn/T/ipykernel_73588/350495278.py:58: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.ChatInterface(fn=chat_with_ai, chatbot=gr.Chatbot())\n",
      "c:\\Users\\Takayama Ryuji\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://a6aed4215b1e517d29.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a6aed4215b1e517d29.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import VectorDBQA, RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import gradio as gr\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "class Document:\n",
    "    def __init__(self, text, metadata):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹ã¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰\n",
    "dir_path = 'loadtext2'\n",
    "\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(dir_path):\n",
    "    file_path = os.path.join(dir_path, filename)\n",
    "    metadata = {'filename': filename}\n",
    "\n",
    "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    if filename.endswith('.pdf'):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            text = ''\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "            documents.append(Document(text, metadata))\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    elif filename.endswith('.txt'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            documents.append(Document(text, metadata))\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²ã¨ãƒ™ã‚¯ãƒˆãƒ«DBã®ä½œæˆ\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# QAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())\n",
    "\n",
    "# ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…\n",
    "def chat_with_ai(input_text, history):\n",
    "    response = qa({\"query\": input_text+'æ¤œç´¢ã§ããŸç¯„å›²ã§ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢ã§ããªã„ã‚‚ã®ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚'})\n",
    "    return response['result']\n",
    "\n",
    "# Gradioã®ChatInterfaceã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.ChatInterface(fn=chat_with_ai, chatbot=gr.Chatbot())\n",
    "    gr.Markdown(\"### AI Chat with Gradio\")\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfrYF6ow3Du/BqoD+S8ar9",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
