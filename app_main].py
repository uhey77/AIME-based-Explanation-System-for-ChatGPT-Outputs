import os
import re
import time
import logging
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, field
import networkx as nx
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import google.generativeai as genai
from google.api_core.exceptions import GoogleAPIError, RetryError
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import gradio as gr
from dotenv import load_dotenv
import io
from PIL import Image

# --- Configuration & Setup ---
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Constants ---
MAX_LLM_RETRIES = 3
NODE_SUMMARY_LENGTH = 25
RETRY_DELAY_SECONDS = 5 # åŸºæœ¬çš„ãªãƒªãƒˆãƒ©ã‚¤é…å»¶
MAX_SEARCH_ITERATIONS = 5

# --- Japanese Font Setup for Matplotlib ---
def setup_japanese_font_for_matplotlib() -> Optional[str]:
    """
    ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¢ã—ã€Matplotlibã«è¨­å®šã‚’è©¦ã¿ã¾ã™ã€‚
    è¦‹ã¤ã‹ã£ãŸãƒ•ã‚©ãƒ³ãƒˆåã‚’è¿”ã—ã¾ã™ã€‚
    """
    font_candidates = [
        'Yu Gothic', 'Yu Mincho', 'MS Gothic', 'MS Mincho', 'Meiryo',  # Windows Standard
        'Hiragino Sans', 'Hiragino Mincho ProN',  # macOS Standard
        'IPAexGothic', 'IPAexMincho',  # Common Free Japanese Fonts
        'Noto Sans CJK JP', 'Noto Serif CJK JP'  # Google Noto Fonts
    ]

    # Matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†æ§‹ç¯‰ (èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘)
    # Note: fm.fontManager.findfont()ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã™ã‚‹å‰¯ä½œç”¨ã‚’æŒã¤ãŸã‚ã€
    # _rebuild()ã®æ˜ç¤ºçš„ãªå‘¼ã³å‡ºã—ã¯Matplotlibã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„ç’°å¢ƒã«ã‚ˆã£ã¦ã¯ä¸è¦ã€ã‚ã‚‹ã„ã¯å•é¡Œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
    # ã—ã‹ã—ã€å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯è©¦ã™ä¾¡å€¤ãŒã‚ã‚‹ã€‚
    try:
        # _rebuild() ã®å‘¼ã³å‡ºã—ã¯ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¾å­˜æ€§ãŒã‚ã‚‹ã®ã§ã€å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¾ãŸã¯try-exceptã§å›²ã‚€
        # æœ€æ–°ã®Matplotlibã§ã¯éæ¨å¥¨ã¾ãŸã¯å­˜åœ¨ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚ã€æ³¨æ„
        if hasattr(fm, '_rebuild'):
            fm._rebuild()
            logger.info("Matplotlib font cache rebuild initiated (if supported).")
    except Exception as e:
        logger.warning(f"Error attempting to rebuild font cache (fm._rebuild()): {e}. This might be normal for your Matplotlib version.")

    available_fonts = {f.name for f in fm.fontManager.ttflist}

    for font_name in font_candidates:
        if font_name in available_fonts:
            try:
                font_prop = fm.FontProperties(family=font_name)
                # findfontã§å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã€‚fallback_to_default=Falseã¯é‡è¦ã§ã€
                # ãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚©ãƒ³ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã•ã›ãªã„ã“ã¨ã§ã€
                # True FontãŒè¦‹ã¤ã‹ã£ãŸã‹ã©ã†ã‹ã®ãƒã‚§ãƒƒã‚¯ã‚’å³å¯†ã«ã™ã‚‹ã€‚
                fm.findfont(font_prop, fallback_to_default=False)
                
                # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®sans-serifã‹ã‚‰DejaVu Sansã‚’å‰Šé™¤ã—ã€
                # è¦‹ã¤ã‹ã£ãŸæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æœ€å„ªå…ˆã«ã™ã‚‹ã€‚
                plt.rcParams['font.family'] = font_name
                # Sans-serifãƒªã‚¹ãƒˆã‹ã‚‰DejaVu Sansã‚’å‰Šé™¤ã—ã€è¦‹ã¤ã‹ã£ãŸãƒ•ã‚©ãƒ³ãƒˆã‚’æœ€åˆã«è¿½åŠ 
                current_sans_serif = [f for f in plt.rcParams.get('font.sans-serif', []) if f != 'DejaVu Sans']
                plt.rcParams['font.sans-serif'] = [font_name] + current_sans_serif
                
                logger.info(f"Found and set usable Japanese font: '{font_name}' globally for Matplotlib. Removed DejaVu Sans from default sans-serif list.")
                return font_name
            except Exception as e:
                logger.debug(f"Font '{font_name}' in ttflist but not usable or error during setting: {e}")
        else:
            logger.debug(f"Font '{font_name}' not in ttflist.")

    logger.warning(
        "No standard Japanese font found or usable by Matplotlib. "
        "Graph labels may not display Japanese characters correctly. "
        "Please install a Japanese font like 'IPAexGothic' or 'Noto Sans CJK JP'."
    )
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®sans-serifã‚’ç¶­æŒ
    # plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Verdana'] + plt.rcParams.get('font.sans-serif', [])
    return None

# Attempt to set a Japanese font globally when the module loads
INSTALLED_JAPANESE_FONT = setup_japanese_font_for_matplotlib()


@dataclass
class ParsedStep:
    id: int
    type: str
    raw_content: str
    summarized_content: str = ""
    basis_ids: List[int] = field(default_factory=list)
    search_query: Optional[str] = None

@dataclass
class LLMThoughtProcess:
    raw_response: str
    steps: List[ParsedStep] = field(default_factory=list)
    final_answer: str = ""
    error_message: Optional[str] = None


class GeminiHandler:
    """Gemini APIã¨ã®å¯¾è©±ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash-lite"):
        if not api_key:
            raise ValueError("Gemini API key is not set.")
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(
            model_name,
            generation_config={"temperature": 0.3},
        )
        self.model_name = model_name
        self.error_messages: List[str] = []

    def _get_base_prompt_template(self) -> str:
        return """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ã‚ãªãŸã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«èª¬æ˜ã—ãªãŒã‚‰å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¯å¿…ãš `<think>` ã¨ `</think>` ã‚¿ã‚°ã§å›²ã‚“ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
å„æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„:
ã‚¹ãƒ†ãƒƒãƒ—[ç•ªå·]: [ç¨®åˆ¥] å†…å®¹ï¼š[å…·ä½“çš„ãªæ€è€ƒå†…å®¹] (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—X, ã‚¹ãƒ†ãƒƒãƒ—Y)

åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ†ãƒƒãƒ—ç¨®åˆ¥ãƒªã‚¹ãƒˆ:
- å•é¡Œå®šç¾©
- ä»®èª¬æç¤º
- æƒ…å ±åé›† (å†…å®¹ã«ã¯ `[æ¤œç´¢ã‚¯ã‚¨ãƒªï¼šã“ã“ã«æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰]` ã®å½¢å¼ã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨˜è¿°)
- æƒ…å ±åˆ†æ
- æ¤œè¨¼
- ä¸­é–“çµè«–
- è«–ç‚¹
- åè«–
- å‚ç…§ (å†…å®¹ã«ã¯å‚ç…§å…ƒã‚’è¨˜è¿°)
- æœ€çµ‚çµè«–å€™è£œ

æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¨˜è¿°ã—ãŸå¾Œã€`<think>` ã‚¿ã‚°ã®å¤–ã«æœ€çµ‚çš„ãªå›ç­”ã‚’è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã®å†…å®¹ã¯ã€å¾Œã§è¦ç´„ã•ã‚Œã‚‹ã“ã¨ã‚’æ„è­˜ã—ã€å…·ä½“çš„ã‹ã¤ç°¡æ½”ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã¯å‡ºåŠ›å½¢å¼ã®å³æ ¼ãªä¾‹ã§ã™ï¼š
è³ªå•ï¼šæ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ
<think>
ã‚¹ãƒ†ãƒƒãƒ—1: å•é¡Œå®šç¾© å†…å®¹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦è³ªå•ã—ã¦ã„ã‚‹ã€‚
ã‚¹ãƒ†ãƒƒãƒ—2: æƒ…å ±åé›† å†…å®¹ï¼š[æ¤œç´¢ã‚¯ãƒªï¼šæ—¥æœ¬ã®é¦–éƒ½]
ã‚¹ãƒ†ãƒƒãƒ—3: æƒ…å ±åˆ†æ å†…å®¹ï¼šæ¤œç´¢çµæœã«ã‚ˆã‚‹ã¨ã€æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã‚ã‚‹ã€‚ (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—2)
ã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚çµè«–å€™è£œ å†…å®¹ï¼šæ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã‚ã‚‹ã¨å›ç­”ã™ã‚‹ã€‚ (æ ¹æ‹ : ã‚¹ãƒ†ãƒƒãƒ—3)
</think>
æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚

---
ã“ã‚Œã¾ã§ã®æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã®å±¥æ­´ (ã‚‚ã—ã‚ã‚Œã°):
{previous_steps_str}
---
ç›´å‰ã®æ¤œç´¢çµæœ (å¿…è¦ãªå ´åˆã®ã¿å‚ç…§):
{search_results_str}
---

ä¸Šè¨˜ã®å±¥æ­´ã¨æ¤œç´¢çµæœã‚’è¸ã¾ãˆã€æ€è€ƒã‚’ç¶šã‘ã‚‹ã‹ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
æ€è€ƒã‚’ç¶šã‘ã‚‹å ´åˆã¯ã€æ–°ã—ã„`<think>`ãƒ–ãƒ­ãƒƒã‚¯ã§ã€ä»¥å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã¨ã¯é‡è¤‡ã—ãªã„ã‚ˆã†ã«ç¶šãã®ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ï¼š{user_question}
"""

    def generate_response(self, user_question: str, previous_steps: Optional[List[ParsedStep]] = None, search_results: Optional[str] = None):
        """Geminiãƒ¢ãƒ‡ãƒ«ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã—ã€å¿œç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"""
        self.error_messages = [] # Reset errors
        previous_steps_str = self._format_previous_steps(previous_steps) if previous_steps else "ãªã—"
        search_results_str = search_results if search_results else "ãªã—"
        prompt = self._get_base_prompt_template().format(
            user_question=user_question,
            previous_steps_str=previous_steps_str,
            search_results_str=search_results_str
        )

        for attempt in range(MAX_LLM_RETRIES):
            try:
                logger.info(f"Sending request to Gemini (Attempt {attempt + 1}/{MAX_LLM_RETRIES}) Model: {self.model_name}")
                response = self.model.generate_content(prompt)
                response_text = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')) if response.candidates else ""
                logger.info("Received response from Gemini.")

                if self._validate_response_format(response_text):
                    yield response_text
                    return
                
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒªãƒˆãƒ©ã‚¤
                self.error_messages.append(f"Geminiã®å‡ºåŠ›å½¢å¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œä¸­ ({attempt + 1}/{MAX_LLM_RETRIES})...")
                logger.warning(f"Gemini response format error (Attempt {attempt + 1}). Retrying...")
                if attempt < MAX_LLM_RETRIES - 1:
                    yield f"Geminiã®å‡ºåŠ›å½¢å¼ã‚¨ãƒ©ãƒ¼ã€‚å†è©¦è¡Œä¸­ã§ã™ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                    time.sleep(RETRY_DELAY_SECONDS)
                else:
                    self.error_messages.append("Geminiå¿œç­”å½¢å¼ã‚¨ãƒ©ãƒ¼ã€‚æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    yield "ERROR: LLM_FORMAT_ERROR_MAX_RETRIES"
                    return
            except RetryError as e:
                logger.error(f"Gemini API retry error exceeded: {e}")
                self.error_messages.append(f"Gemini APIãƒªãƒˆãƒ©ã‚¤ã‚¨ãƒ©ãƒ¼è¶…éã€‚å†è©¦è¡Œä¸­ ({attempt + 1}/{MAX_LLM_RETRIES})...")
                if attempt < MAX_LLM_RETRIES - 1:
                    yield f"Gemini APIã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤è¶…éï¼‰ã€‚å†è©¦è¡Œä¸­ã§ã™ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                    time.sleep(RETRY_DELAY_SECONDS * (attempt + 1))
                else:
                    self.error_messages.append("Gemini APIã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤è¶…éï¼‰ã€‚æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    yield "ERROR: API_RETRY_ERROR_MAX_RETRIES"
                    return
            except GoogleAPIError as e:
                logger.error(f"Gemini API error: {e}")
                error_detail = ""
                if "API key not valid" in str(e) or (hasattr(e, 'grpc_status_code') and e.grpc_status_code == 7):
                    error_detail = "APIã‚­ãƒ¼ãŒç„¡åŠ¹ã‹ã€æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                    self.error_messages.append(error_detail)
                    yield "ERROR: API_KEY_INVALID"
                    return
                elif "Rate limit exceeded" in str(e):
                    # ã‚¯ã‚©ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚ˆã‚Šé•·ã„é…å»¶ã‚’è€ƒæ…®
                    retry_after_seconds = 0
                    if hasattr(e, 'error_info') and e.error_info and e.error_info.quota_violations:
                        for violation in e.error_info.quota_violations:
                            if hasattr(violation, 'retry_delay') and hasattr(violation.retry_delay, 'seconds'):
                                retry_after_seconds = max(retry_after_seconds, violation.retry_delay.seconds)
                    
                    if retry_after_seconds > 0:
                        error_detail = f"Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚{retry_after_seconds}ç§’å¾Œã«å†è©¦è¡Œã—ã¾ã™ã€‚"
                        self.error_messages.append(error_detail)
                        logger.warning(error_detail)
                        yield f"Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                        if attempt < MAX_LLM_RETRIES - 1:
                            time.sleep(retry_after_seconds + 5) 
                        else:
                            yield "ERROR: API_RATE_LIMIT"
                            return
                    else: 
                        error_detail = "Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
                        self.error_messages.append(error_detail)
                        yield f"Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                        if attempt < MAX_LLM_RETRIES - 1:
                            time.sleep(RETRY_DELAY_SECONDS * (attempt + 2)) 
                        else:
                            yield "ERROR: API_RATE_LIMIT"
                            return
                else:
                    error_detail = "ä¸æ˜ãªAPIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
                    self.error_messages.append(f"Gemini APIã‚¨ãƒ©ãƒ¼: {error_detail}")
                    if attempt < MAX_LLM_RETRIES - 1:
                        yield f"Gemini APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œä¸­ã§ã™ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                        time.sleep(RETRY_DELAY_SECONDS)
                    else:
                        yield "ERROR: API_ERROR_MAX_RETRIES"
                        return
            except Exception as e:
                logger.error(f"Unexpected error during Gemini API call: {e}")
                self.error_messages.append(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œä¸­ ({attempt + 1}/{MAX_LLM_RETRIES})...")
                if attempt < MAX_LLM_RETRIES - 1:
                    yield f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œä¸­ã§ã™ ({attempt + 2}/{MAX_LLM_RETRIES})..."
                    time.sleep(RETRY_DELAY_SECONDS)
                else:
                    self.error_messages.append("äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ã€‚æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    yield "ERROR: UNEXPECTED_ERROR_MAX_RETRIES"
                    return
        self.error_messages.append("ä¸æ˜ãªã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€å¿œç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        yield "ERROR: UNKNOWN_FAILURE_AFTER_RETRIES"

    def _format_previous_steps(self, steps: List[ParsedStep]) -> str:
        """éå»ã®æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢ã—ã¾ã™ã€‚"""
        if not steps:
            return "ãªã—"
        return "\n".join([f"ã‚¹ãƒ†ãƒƒãƒ—{s.id}: {s.type} å†…å®¹ï¼š{s.raw_content}" + (f" (æ ¹æ‹ : {', '.join(map(str, s.basis_ids))})" if s.basis_ids else "") for s in steps])

    def _validate_response_format(self, response_text: str) -> bool:
        """LLMã®å¿œç­”ãŒæœŸå¾…ã•ã‚Œã‚‹å½¢å¼ã§ã‚ã‚‹ã‹æ¤œè¨¼ã—ã¾ã™ã€‚"""
        has_think_tags = "<think>" in response_text and "</think>" in response_text
        if not response_text.strip():
            logger.warning("Validation failed: Response text is empty.")
            return False
        if not has_think_tags:
            logger.warning("Validation failed: Missing <think> or </think> tags.")
            return False
        return True

    def summarize_text(self, text_to_summarize: str, max_length: int = NODE_SUMMARY_LENGTH) -> str:
        """ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¾ã™ã€‚"""
        if not text_to_summarize or not text_to_summarize.strip():
            return "ï¼ˆç©ºã®å†…å®¹ï¼‰"
        try:
            prompt = f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã§{max_length}æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã€æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ‰ãˆã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚:\n\n\"{text_to_summarize}\""
            summary_model = genai.GenerativeModel(self.model_name)
            response = summary_model.generate_content(prompt)
            summary = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')).strip() if response.candidates else ""

            if not summary:
                logger.warning(f"Summarization resulted in empty text for: '{text_to_summarize[:50]}...'")
                return text_to_summarize[:max_length] + "..." if len(text_to_summarize) > max_length else text_to_summarize
            return summary if len(summary) <= max_length + 10 else summary[:max_length+5] + "..."
        except Exception as e:
            # summarize_textå†…ã§ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼ã¯å‘¼ã³å‡ºã—å…ƒã§ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã‚ˆã†ã€ã“ã“ã§ã¯å†raiseã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¼é”
            # GraphGeneratorã§ã“ã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒã—ã€ã‚°ãƒ©ãƒ•ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å«ã‚ã‚‹
            raise e # ã‚¨ãƒ©ãƒ¼ã‚’å†ã‚¹ãƒ­ãƒ¼ã—ã¦å‘¼ã³å‡ºã—å…ƒã§ã‚­ãƒ£ãƒƒãƒã•ã›ã‚‹

    def get_error_messages(self) -> List[str]:
        """GeminiHandlerå†…ã§ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚"""
        return self.error_messages


class GoogleSearchHandler:
    """Google Custom Search APIã‚’ä»‹ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, api_key: str, cse_id: str):
        self.service = None
        self.is_enabled = False
        if not api_key or not cse_id:
            logger.warning("Google API key or CSE ID is not set. Search functionality will be disabled.")
            return
        try:
            self.service = build("customsearch", "v1", developerKey=api_key)
            self.cse_id = cse_id
            self.is_enabled = True
            logger.info("Google Search Handler initialized.")
        except Exception as e:
            logger.error(f"Failed to initialize Google Search service: {e}")

    def search(self, query: str, num_results: int = 3) -> Optional[str]:
        """æŒ‡å®šã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§Googleæ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã‚’æ•´å½¢ã—ã¦è¿”ã—ã¾ã™ã€‚"""
        if not self.is_enabled or not self.service:
            logger.warning("Google Search service not initialized or disabled. Skipping search.")
            return "æ¤œç´¢æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚APIã‚­ãƒ¼ã¾ãŸã¯CSE IDã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        try:
            logger.info(f"Performing Google search for: '{query}'")
            res = self.service.cse().list(q=query, cx=self.cse_id, num=num_results, lr='lang_ja').execute()
            if 'items' not in res or not res['items']:
                logger.info(f"No search results found for query: '{query}'")
                return "æ¤œç´¢çµæœãªã—ã€‚"
            search_results_text = "æ¤œç´¢çµæœ:\n"
            for i, item in enumerate(res['items']):
                title = item.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
                snippet = item.get('snippet', 'ã‚¹ãƒ‹ãƒšãƒƒãƒˆãªã—').replace("\n", " ").strip()
                link = item.get('link', '#')
                search_results_text += f"{i+1}. ã‚¿ã‚¤ãƒˆãƒ«: {title}\n  ã‚¹ãƒ‹ãƒšãƒƒãƒˆ: {snippet}\n  URL: {link}\n\n"
            logger.info(f"Search successful for query: '{query}'")
            return search_results_text.strip()
        except HttpError as e:
            logger.error(f"Google Search API HTTP error: {e.resp.status} {e._get_reason()} for query '{query}'")
            return f"æ¤œç´¢APIã‚¨ãƒ©ãƒ¼: {e._get_reason()}"
        except Exception as e:
            logger.error(f"An unexpected error occurred during Google search for '{query}': {e}")
            return "æ¤œç´¢ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"


class ThoughtParser:
    """LLMã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def parse_llm_output(self, raw_llm_output: str) -> LLMThoughtProcess:
        """LLMã®ç”Ÿã®å‡ºåŠ›ã‚’è§£æã—ã€æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã¨æœ€çµ‚å›ç­”ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
        result = LLMThoughtProcess(raw_response=raw_llm_output)
        think_match = re.search(r"<think>(.*?)</think>", raw_llm_output, re.DOTALL)

        if not think_match:
            result.error_message = "LLMã®å¿œç­”ã«<think>ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            result.final_answer = raw_llm_output.strip()
            return result

        think_content = think_match.group(1).strip()
        result.final_answer = raw_llm_output[think_match.end():].strip()

        step_pattern = re.compile(
            r"ã‚¹ãƒ†ãƒƒãƒ—\s*(\d+)\s*:\s*([^å†…å®¹]+?)\s*å†…å®¹ï¼š(.*?)(?:\s*\(æ ¹æ‹ :\s*(ã‚¹ãƒ†ãƒƒãƒ—\s*\d+(?:,\s*ã‚¹ãƒ†ãƒƒãƒ—\s*\d+)*)\s*\))?$",
            re.MULTILINE | re.IGNORECASE
        )
        parsed_steps_in_segment = []
        for match in step_pattern.finditer(think_content):
            try:
                step_id_from_llm = int(match.group(1))
                step_type = match.group(2).strip()
                raw_content = match.group(3).strip()
                basis_str = match.group(4)
                basis_ids = [int(b_id) for b_id in re.findall(r'ã‚¹ãƒ†ãƒƒãƒ—\s*(\d+)', basis_str, re.IGNORECASE)] if basis_str else []
                search_query_match = re.search(r"\[æ¤œç´¢ã‚¯ã‚¨ãƒªï¼š(.*?)\]", raw_content)
                search_query = search_query_match.group(1).strip() if search_query_match else None

                parsed_steps_in_segment.append(ParsedStep(
                    id=step_id_from_llm, type=step_type, raw_content=raw_content,
                    basis_ids=basis_ids, search_query=search_query
                ))
            except Exception as e:
                msg = f"ã‚¹ãƒ†ãƒƒãƒ—è§£æã‚¨ãƒ©ãƒ¼: {match.group(0)} ({e})"
                logger.error(msg)
                result.error_message = (result.error_message or "") + "\n" + msg

        result.steps = parsed_steps_in_segment
        if not result.steps and think_content:
            msg = "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹(<think>ã‚¿ã‚°å†…)ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸãŒã€æœ‰åŠ¹ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            logger.warning(msg + f" Content: {think_content[:100]}...")
            result.error_message = (result.error_message or "") + "\n" + msg
        return result


# (ä»–ã®ã‚¯ãƒ©ã‚¹ã‚„importæ–‡ã¯å…ƒã®ã¾ã¾)
import matplotlib.patches as mpatches
import matplotlib.lines as mlines

class GraphGenerator:
    """
    LLMã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦å¯è¦–åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹ (æœ€çµ‚ãƒ»å®‰å®šç‰ˆ)
    - ãƒãƒ¼ãƒ‰ã‚’æ¨ªé•·ã®é•·æ–¹å½¢ã§è¡¨ç¤º
    - ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ã«ã‚ˆã‚‹éšå±¤çš„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨
    - å³ä¸Šã«å‡¡ä¾‹ã‚’è¡¨ç¤º
    - çŸ¢å°ã‚’å¼·èª¿ã—ã€ãƒãƒ¼ãƒ‰ã¨ã®é‡ãªã‚Šã‚’å›é¿ (ãƒãƒ¼ã‚¸ãƒ§ãƒ³éä¾å­˜ã®æ–¹æ³•ã§å®Ÿè£…)
    """
    def __init__(self, llm_handler: 'GeminiHandler', installed_japanese_font: Optional[str]):
        # ... (å¤‰æ›´ãªã—)
        self.llm_handler = llm_handler
        self.error_messages_for_graph: List[str] = []
        self.font_properties = None

        if installed_japanese_font:
            try:
                self.font_properties = fm.FontProperties(family=installed_japanese_font)
                logger.info(f"GraphGenerator: Using Matplotlib-configured font: '{installed_japanese_font}'.")
            except Exception as e:
                logger.error(f"GraphGenerator: Failed to load font property for '{installed_japanese_font}': {e}")
                self.error_messages_for_graph.append(f"è¨­å®šæ¸ˆã¿æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆ '{installed_japanese_font}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            self.error_messages_for_graph.append("é©åˆ‡ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã®æ–‡å­—ãŒæ­£ã—ãè¡¨ç¤ºã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            logger.warning("GraphGenerator: No specific font set. Relying on Matplotlib's default font mechanisms.")

    def _summarize_step_contents(self, steps: List['ParsedStep'], progress_fn=None):
        # ... (å¤‰æ›´ãªã—)
        logger.info(f"Summarizing contents for {len(steps)} steps...")
        if not steps: return
        for i, step in enumerate(steps):
            if progress_fn:
                progress_fn((i + 1) / len(steps), f"ã‚¹ãƒ†ãƒƒãƒ— {step.id} ã®å†…å®¹ã‚’è¦ç´„ä¸­ ({i+1}/{len(steps)})...")
            if not step.summarized_content:
                try:
                    step.summarized_content = self.llm_handler.summarize_text(step.raw_content, max_length=NODE_SUMMARY_LENGTH)
                except Exception as e:
                    error_msg = f"ã‚¹ãƒ†ãƒƒãƒ—S{step.id}ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
                    logger.error(error_msg)
                    self.error_messages_for_graph.append(error_msg)
                    step.summarized_content = step.raw_content[:NODE_SUMMARY_LENGTH] + "..." if len(step.raw_content) > NODE_SUMMARY_LENGTH else step.raw_content
            time.sleep(0.05)
        logger.info("Summarization complete.")
        
    def _custom_hierarchical_layout(self, G, root_node=0, vertical_gap=0.5, horizontal_gap=0.5):
        # ... (å¤‰æ›´ãªã—)
        if not root_node in G:
            logger.warning(f"Root node {root_node} not in graph. Cannot create hierarchical layout.")
            return None
        levels = {root_node: 0}
        nodes_at_level = {0: [root_node]}
        queue = [root_node]
        visited = {root_node}
        max_level = 0
        while queue:
            parent = queue.pop(0)
            parent_level = levels[parent]
            children = sorted(list(G.successors(parent)), key=lambda n: G.get_edge_data(parent, n).get('type') != 'sequential')
            for child in children:
                if child not in visited:
                    visited.add(child)
                    child_level = parent_level + 1
                    levels[child] = child_level
                    if child_level not in nodes_at_level:
                        nodes_at_level[child_level] = []
                    nodes_at_level[child_level].append(child)
                    queue.append(child)
                    max_level = max(max_level, child_level)
        all_nodes = set(G.nodes())
        unvisited = all_nodes - visited
        if unvisited:
            unvisited_level = max_level + 1
            nodes_at_level[unvisited_level] = list(unvisited)
            for node in unvisited:
                levels[node] = unvisited_level
        pos = {}
        for level, nodes in nodes_at_level.items():
            num_nodes = len(nodes)
            level_width = (num_nodes - 1) * horizontal_gap
            x_start = -level_width / 2
            sorted_nodes = sorted(nodes)
            for i, node in enumerate(sorted_nodes):
                x = x_start + i * horizontal_gap
                y = -level * vertical_gap
                pos[node] = (x, y)
        return pos

    def create_thinking_graph(self, user_question: str, all_steps: List['ParsedStep'], final_answer_text: str, progress_fn=None) -> Optional[plt.Figure]:
        # ... (ã‚°ãƒ©ãƒ•æ§‹ç¯‰éƒ¨åˆ†ã¯å¤‰æ›´ãªã—) ...
        self.error_messages_for_graph = []
        llm_errors = self.llm_handler.get_error_messages()
        if llm_errors: self.error_messages_for_graph.extend(llm_errors)
        self._summarize_step_contents(all_steps, progress_fn)
        G = nx.DiGraph()
        QUESTION_NODE_ID = 0
        try:
            question_summary = self.llm_handler.summarize_text(user_question, max_length=NODE_SUMMARY_LENGTH + 15)
        except Exception as e:
            error_msg = f"è³ªå•ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"; logger.error(error_msg); self.error_messages_for_graph.append(error_msg)
            question_summary = user_question[:NODE_SUMMARY_LENGTH + 15] + "..."
        G.add_node(QUESTION_NODE_ID, label=f"è³ªå•:\n{question_summary}", type="question", color="skyblue")
        valid_step_ids_in_graph = {QUESTION_NODE_ID}
        current_prev_node_id = QUESTION_NODE_ID
        if all_steps:
            max_existing_id = max(s.id for s in all_steps) if all_steps else 0
            for step in sorted(all_steps, key=lambda s: s.id):
                node_graph_id = step.id
                if node_graph_id in G: continue
                label_text = f"S{step.id}: {step.type}\n{step.summarized_content}"
                G.add_node(node_graph_id, label=label_text, type="ai_step", color="khaki")
                valid_step_ids_in_graph.add(node_graph_id)
                if current_prev_node_id in G: G.add_edge(current_prev_node_id, node_graph_id, type="sequential", style="solid", color="gray")
                current_prev_node_id = node_graph_id
            try:
                answer_summary = self.llm_handler.summarize_text(final_answer_text, max_length=NODE_SUMMARY_LENGTH + 15)
            except Exception as e:
                error_msg = f"æœ€çµ‚å›ç­”ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"; logger.error(error_msg); self.error_messages_for_graph.append(error_msg)
                answer_summary = final_answer_text[:NODE_SUMMARY_LENGTH + 15] + "..."
            final_answer_node_id = max_existing_id + 1
            G.add_node(final_answer_node_id, label=f"æœ€çµ‚å›ç­”:\n{answer_summary}", type="final_answer", color="lightgreen")
            valid_step_ids_in_graph.add(final_answer_node_id)
            if current_prev_node_id != QUESTION_NODE_ID and current_prev_node_id in G: G.add_edge(current_prev_node_id, final_answer_node_id, type="sequential", style="solid", color="gray")
            elif QUESTION_NODE_ID in G: G.add_edge(QUESTION_NODE_ID, final_answer_node_id, type="sequential", style="solid", color="gray")
            for step in all_steps:
                target_node_id = step.id
                for basis_id in step.basis_ids:
                    if basis_id in valid_step_ids_in_graph and target_node_id in valid_step_ids_in_graph and basis_id != target_node_id:
                        if not G.has_edge(basis_id, target_node_id) or G.get_edge_data(basis_id, target_node_id).get('type') != 'sequential': G.add_edge(basis_id, target_node_id, type="basis", style="dashed", color="purple")
        elif final_answer_text and QUESTION_NODE_ID in G:
            try:
                answer_summary = self.llm_handler.summarize_text(final_answer_text, max_length=NODE_SUMMARY_LENGTH + 15)
            except Exception as e:
                error_msg = f"æœ€çµ‚å›ç­”ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"; logger.error(error_msg); self.error_messages_for_graph.append(error_msg)
                answer_summary = final_answer_text[:NODE_SUMMARY_LENGTH + 15] + "..."
            final_answer_node_id = QUESTION_NODE_ID + 1
            G.add_node(final_answer_node_id, label=f"æœ€çµ‚å›ç­”:\n{answer_summary}", type="final_answer", color="lightgreen")
            G.add_edge(QUESTION_NODE_ID, final_answer_node_id, type="sequential", style="solid", color="gray")
        else:
            self.error_messages_for_graph.append("æœ‰åŠ¹ãªæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¾ãŸã¯å›ç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã¯ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚"); return None
        if not G.nodes():
            self.error_messages_for_graph.append("ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); return None

        fig, ax = plt.subplots(figsize=(min(20, max(12, G.number_of_nodes() * 2.0)), min(15, max(8, G.number_of_nodes() * 1.0))))
        
        pos = None
        try:
            logger.info("Using custom hierarchical layout.")
            pos = self._custom_hierarchical_layout(G, root_node=QUESTION_NODE_ID)
            if pos is None: raise ValueError("Custom layout returned None.")
        except Exception as e:
            logger.warning(f"Custom hierarchical layout failed ('{e}'). Falling back to spring layout.")
            try:
                pos = nx.spring_layout(G, k=0.9, iterations=50, seed=42)
            except Exception as e2:
                logger.error(f"Fallback spring_layout also failed: {e2}. Using random_layout.")
                pos = nx.random_layout(G, seed=42)

        if pos is None:
            self.error_messages_for_graph.append("ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"); plt.close(fig); return None

        labels = {n: d['label'] for n, d in G.nodes(data=True)}
        
        # â˜…â˜…â˜…â˜…â˜… ã“ã“ã‹ã‚‰ãŒå¤‰æ›´ç®‡æ‰€ â˜…â˜…â˜…â˜…â˜…
        # nx.draw_networkx_edgesã‚’å®Œå…¨ã«ã‚„ã‚ã€FancyArrowPatchã§çŸ¢å°ã‚’æ‰‹å‹•æç”»ã™ã‚‹

        # 1. ã‚¨ãƒƒã‚¸ã‚’ä¸€æœ¬ãšã¤æç”»
        for u, v, data in G.edges(data=True):
            start_pos = pos[u]
            end_pos = pos[v]
            
            # ç·šã®ç¨®é¡ã«å¿œã˜ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨­å®š
            if data.get('type') == 'sequential':
                arrow_style = mpatches.ArrowStyle.Simple(head_length=15, head_width=10, tail_width=2.5)
                arrow = mpatches.FancyArrowPatch(
                    start_pos, end_pos,
                    arrowstyle=arrow_style,
                    color="black",
                    shrinkA=0, shrinkB=20, # çµ‚ç‚¹ã‚’20ãƒã‚¤ãƒ³ãƒˆç¸®ã‚ã‚‹
                    mutation_scale=1,
                    connectionstyle='arc3,rad=0.05'
                )
            elif data.get('type') == 'basis':
                arrow_style = mpatches.ArrowStyle.Simple(head_length=10, head_width=6, tail_width=0.8)
                arrow = mpatches.FancyArrowPatch(
                    start_pos, end_pos,
                    arrowstyle=arrow_style,
                    color="purple",
                    linestyle='dashed',
                    shrinkA=0, shrinkB=20, # çµ‚ç‚¹ã‚’20ãƒã‚¤ãƒ³ãƒˆç¸®ã‚ã‚‹
                    mutation_scale=1,
                    connectionstyle='arc3,rad=0.15'
                )
            else:
                continue # æœªçŸ¥ã®ã‚¿ã‚¤ãƒ—ã¯æç”»ã—ãªã„
            
            ax.add_patch(arrow)

        # 2. ãƒãƒ¼ãƒ‰ã‚’æç”» (å¤‰æ›´ãªã—)
        node_width, node_height = 0.4, 0.2
        for node in G.nodes():
            x_center, y_center = pos[node]
            node_color = G.nodes[node]['color']
            x_corner, y_corner = x_center - node_width / 2, y_center - node_height / 2
            rect = plt.Rectangle((x_corner, y_corner), node_width, node_height, facecolor=node_color, alpha=0.95, transform=ax.transData)
            ax.add_patch(rect)
        
        # 3. ãƒ©ãƒ™ãƒ«ã‚’æç”» (å¤‰æ›´ãªã—)
        nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, ax=ax, font_weight='bold', verticalalignment='center')
        
        # 4. å‡¡ä¾‹ã‚’æç”» (å¤‰æ›´ãªã—)
        node_handles = [
            mpatches.Patch(color='skyblue', label='è³ªå•'),
            mpatches.Patch(color='khaki', label='AIã®æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—'),
            mpatches.Patch(color='lightgreen', label='æœ€çµ‚å›ç­”')
        ]
        edge_handles = [
            mlines.Line2D([], [], color='black', linestyle='solid', label='æ™‚ç³»åˆ—ã®æµã‚Œ'),
            mlines.Line2D([], [], color='purple', linestyle='dashed', label='æ ¹æ‹ ãƒ»å‚ç…§')
        ]
        all_handles = node_handles + edge_handles
        ax.legend(handles=all_handles, loc='upper right', fontsize='small', title="å‡¡ä¾‹", prop=self.font_properties)
        
        # â˜…â˜…â˜…â˜…â˜… å¤‰æ›´ç®‡æ‰€ã“ã“ã¾ã§ â˜…â˜…â˜…â˜…â˜…

        ax.set_title("AIæ€è€ƒé€£é–ã®å¯è¦–åŒ– (Gemini)", fontsize=16, fontweight='bold', fontproperties=self.font_properties if self.font_properties else None)
        
        ax.autoscale()
        ax.margins(0.1) 
        plt.axis('off')
        plt.tight_layout(pad=1.0)
        return fig

    def get_error_messages_for_graph(self) -> List[str]:
        # ... (å¤‰æ›´ãªã—)
        return self.error_messages_for_graph

    def get_error_messages_html(self) -> str:
        # ... (å¤‰æ›´ãªã—)
        if not self.error_messages_for_graph: return ""
        escaped_messages = [msg.replace("&", "&").replace("<", "<").replace(">", ">") for msg in self.error_messages_for_graph]
        html = "<div style='color: red; border: 1px solid red; padding: 10px; margin-top: 10px;'>" \
               "<strong>ã‚°ãƒ©ãƒ•ã«é–¢ã™ã‚‹æ³¨æ„:</strong><ul>"
        for msg in escaped_messages:
            html += f"<li>{msg}</li>"
        html += "</ul></div>"
        return html


class AISystem:
    """AIæ€è€ƒã‚·ã‚¹ãƒ†ãƒ ã®ä¸­æ ¸ã‚’æ‹…ã†ã‚¯ãƒ©ã‚¹ã€‚LLMã¨æ¤œç´¢æ©Ÿèƒ½ã‚’çµ±åˆã—ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã—ã¾ã™ã€‚"""
    def __init__(self):
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        # å¤‰æ•°åã®ã‚¿ã‚¤ãƒä¿®æ­£ Google Search_api_key -> Google Search_api_key
        self.Google_Search_api_key = os.getenv("GOOGLE_API_KEY") 
        self.google_cse_id = os.getenv("GOOGLE_CSE_ID")

        if not self.gemini_api_key:
            logger.error("CRITICAL: GEMINI_API_KEY environment variable not set.")

        self.llm_handler = GeminiHandler(api_key=self.gemini_api_key)
        self.search_handler = GoogleSearchHandler(api_key=self.Google_Search_api_key, cse_id=self.google_cse_id)
        self.parser = ThoughtParser()
        self.graph_generator = GraphGenerator(llm_handler=self.llm_handler, installed_japanese_font=INSTALLED_JAPANESE_FONT)

    def _append_and_renumber_steps(self, existing_steps: List[ParsedStep], new_steps_from_llm: List[ParsedStep]) -> List[ParsedStep]:
        """æ—¢å­˜ã®ã‚¹ãƒ†ãƒƒãƒ—ã«æ–°ã—ã„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¿½åŠ ã—ã€é‡è¤‡ã‚’æ’é™¤ã—ã¤ã¤IDã‚’æŒ¯ã‚Šç›´ã—ã¾ã™ã€‚"""
        updated_all_steps = list(existing_steps)
        existing_signatures = {(step.type, step.raw_content) for step in existing_steps}
        
        current_max_id = max(s.id for s in existing_steps) if existing_steps else 0

        for new_step in new_steps_from_llm:
            if (new_step.type, new_step.raw_content) in existing_signatures:
                logger.info(f"Skipping duplicate step (content match): Type='{new_step.type}', Content='{new_step.raw_content[:30]}...'")
                continue
            
            current_max_id += 1
            new_step.id = current_max_id
            updated_all_steps.append(new_step)
            existing_signatures.add((new_step.type, new_step.raw_content))
            
        return updated_all_steps

    def process_question_iterations(self, user_question: str, progress_fn):
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’åå¾©çš„ã«å®Ÿè¡Œã—ã€
        æœ€çµ‚å›ç­”ã€æ€è€ƒã‚°ãƒ©ãƒ•ã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        """
        if not self.gemini_api_key:
            yield ("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None, "<p style='color:red; font-weight:bold;'>Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚</p>")
            return

        progress_fn(0, desc="AI(Gemini)ãŒæ€è€ƒã‚’é–‹å§‹ã—ã¾ã—ãŸ...")
        accumulated_steps: List[ParsedStep] = []
        accumulated_final_answer: str = ""
        current_raw_llm_output_segment = ""
        search_iteration_count = 0
        overall_error_messages: List[str] = [] # å…¨ä½“ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åé›†

        # 1. æœ€åˆã®LLMå¿œç­”
        llm_response_generator = self.llm_handler.generate_response(user_question)
        for update_message in llm_response_generator:
            if "ERROR:" in update_message:
                current_raw_llm_output_segment = update_message
                break
            if update_message.endswith("..."):
                progress_fn(0.1, desc=update_message)
            else:
                current_raw_llm_output_segment = update_message

        overall_error_messages.extend(self.llm_handler.get_error_messages())

        if "ERROR:" in current_raw_llm_output_segment:
            error_msg = f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {current_raw_llm_output_segment.replace('ERROR: ', '')}"
            yield (error_msg, None, f"<p style='color:red; font-weight:bold;'>{error_msg}</p>"); return

        parsed_segment_obj = self.parser.parse_llm_output(current_raw_llm_output_segment)
        accumulated_steps = self._append_and_renumber_steps(accumulated_steps, parsed_segment_obj.steps)
        accumulated_final_answer = parsed_segment_obj.final_answer
        if parsed_segment_obj.error_message:
            overall_error_messages.append(f"ãƒ‘ãƒ¼ã‚µãƒ¼ã‹ã‚‰ã®é€šçŸ¥: {parsed_segment_obj.error_message}")

        last_parsed_segment_steps = parsed_segment_obj.steps

        # 2. æ¤œç´¢ã¨LLMã®åå¾©å‡¦ç†
        while any(s.type == "æƒ…å ±åé›†" and s.search_query for s in last_parsed_segment_steps) and \
              search_iteration_count < MAX_SEARCH_ITERATIONS:
            search_step = next((s for s in last_parsed_segment_steps if s.type == "æƒ…å ±åé›†" and s.search_query), None)
            if not search_step:
                break

            search_iteration_count += 1
            query = search_step.search_query
            prog_frac = min(0.3 + search_iteration_count * 0.1, 0.9)
            progress_fn(prog_frac, desc=f"'{query[:30]}...'æ¤œç´¢ä¸­({search_iteration_count}/{MAX_SEARCH_ITERATIONS})...")
            
            s_results = self.search_handler.search(query) or "æƒ…å ±å–å¾—å¤±æ•—ã€‚"
            
            prog_frac = min(0.4 + search_iteration_count * 0.1, 0.9)
            progress_fn(prog_frac, desc="æ¤œç´¢çµæœã‚’å…ƒã«AI(Gemini)ãŒå†è€ƒä¸­...")

            llm_response_generator = self.llm_handler.generate_response(user_question, accumulated_steps, s_results)
            current_raw_llm_output_segment = ""
            for update_message in llm_response_generator:
                if "ERROR:" in update_message:
                    current_raw_llm_output_segment = update_message
                    break
                if update_message.endswith("..."):
                    prog_frac_inner = min(0.5 + search_iteration_count * 0.1, 0.9)
                    progress_fn(prog_frac_inner, desc=update_message)
                else:
                    current_raw_llm_output_segment = update_message
            
            overall_error_messages.extend(self.llm_handler.get_error_messages())

            if "ERROR:" in current_raw_llm_output_segment:
                error_msg = f"AIå‡¦ç†ã‚¨ãƒ©ãƒ¼(æ¤œç´¢å¾Œ): {current_raw_llm_output_segment.replace('ERROR: ', '')}"
                overall_error_messages.append(error_msg)
                break

            parsed_segment_obj = self.parser.parse_llm_output(current_raw_llm_output_segment)
            accumulated_steps = self._append_and_renumber_steps(accumulated_steps, parsed_segment_obj.steps)
            if parsed_segment_obj.final_answer.strip():
                accumulated_final_answer = parsed_segment_obj.final_answer
            if parsed_segment_obj.error_message:
                overall_error_messages.append(f"ãƒ‘ãƒ¼ã‚µãƒ¼é€šçŸ¥(æ¤œç´¢å¾Œ): {parsed_segment_obj.error_message}")
            
            last_parsed_segment_steps = parsed_segment_obj.steps
            if not any(s.type == "æƒ…å ±åé›†" and s.search_query for s in last_parsed_segment_steps):
                break
        
        # æœ€çµ‚å›ç­”ã®èª¿æ•´
        if not accumulated_final_answer.strip() and accumulated_steps and not ("ERROR:" in current_raw_llm_output_segment):
            accumulated_final_answer = "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã«åŸºã¥ãæ¤œè¨ã—ã¾ã—ãŸãŒã€æœ€çµ‚å›ç­”ã®æ˜ç¤ºçš„å‡ºåŠ›ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚°ãƒ©ãƒ•ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
        elif not accumulated_final_answer.strip() and not accumulated_steps and not ("ERROR:" in current_raw_llm_output_segment):
            accumulated_final_answer = "AIã‹ã‚‰ã®æœ‰åŠ¹ãªå¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        progress_fn(0.8, desc="æ€è€ƒã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­ã§ã™...")
        summary_prog_lambda = lambda p, d: progress_fn(0.8 + p * 0.15, desc=d)
        
        fig = self.graph_generator.create_thinking_graph(user_question, accumulated_steps, accumulated_final_answer, summary_prog_lambda)
        
        graph_image_pil = None
        if fig:
            try:
                buf = io.BytesIO()
                fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)
                plt.close(fig)
                buf.seek(0)
                graph_image_pil = Image.open(buf)
            except Exception as e:
                logger.error(f"Error converting graph to PIL Image or saving: {e}")
                overall_error_messages.append("è­¦å‘Š: ã‚°ãƒ©ãƒ•ç”»åƒã®ç”Ÿæˆã¾ãŸã¯è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        overall_error_messages.extend(self.graph_generator.get_error_messages_for_graph())
        graph_errors_html = self._format_overall_error_messages(overall_error_messages)

        progress_fn(1.0, desc="å®Œäº†")
        yield (accumulated_final_answer, graph_image_pil, graph_errors_html)

    def _format_overall_error_messages(self, messages: List[str]) -> str:
        """å…¨ã¦ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’HTMLå½¢å¼ã§æ•´å½¢ã—ã¾ã™ã€‚"""
        if not messages:
            return ""
        unique_messages = sorted(list(set(messages)))
        html_content = "<div style='color: red; border: 1px solid red; padding: 10px; margin-top: 10px;'>"\
                       "<strong>å‡¦ç†ã«é–¢ã™ã‚‹æ³¨æ„ãƒ»ã‚¨ãƒ©ãƒ¼:</strong><ul>"
        for msg in unique_messages:
            html_content += f"<li>{msg}</li>"
        html_content += "</ul></div>"
        return html_content


def create_gradio_interface(system: AISystem):
    """Gradio UIã‚’ä½œæˆã—ã¾ã™ã€‚"""
    with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.sky)) as demo:
        gr.Markdown("# ğŸ§  AIæ€è€ƒé€£é–å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ  (Geminiç‰ˆ)")
        gr.Markdown("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã€AI(Gemini)ãŒæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«ç¤ºã—ãªãŒã‚‰å›ç­”ã—ã¾ã™ã€‚æ€è€ƒã®é€£é–ã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ã¨ã—ã¦å¯è¦–åŒ–ã•ã‚Œã¾ã™ã€‚")

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(label="è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", placeholder="ä¾‹: æ—¥æœ¬ã®AIæŠ€è¡“ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãã®èª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ", lines=3, show_copy_button=True)
                submit_button = gr.Button("è³ªå•ã‚’é€ä¿¡ã™ã‚‹", variant="primary")
            with gr.Column(scale=1):
                gr.Markdown(f"### è¨­å®šï¼ˆå‚è€ƒï¼‰\n"
                            f"- LLM Model: `{system.llm_handler.model_name}`\n"
                            f"- Googleæ¤œç´¢: `{'æœ‰åŠ¹' if system.search_handler.is_enabled else 'ç„¡åŠ¹ (APIã‚­ãƒ¼/CSE IDæœªè¨­å®š)'}`\n"
                            f"- æœ€å¤§æ¤œç´¢å›æ•°: `{MAX_SEARCH_ITERATIONS}`\n"
                            f"- LLMå†è©¦è¡Œå›æ•°: `{MAX_LLM_RETRIES}`\n"
                            f"- ä½¿ç”¨ãƒ•ã‚©ãƒ³ãƒˆ(ã‚°ãƒ©ãƒ•): `{INSTALLED_JAPANESE_FONT if INSTALLED_JAPANESE_FONT else '(æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)'}`")
        gr.Markdown("---")
        gr.Markdown("## ğŸ¤– AIã®å›ç­”ã¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹")
        with gr.Row():
            with gr.Column(scale=1):
                answer_output = gr.Markdown(label="AIã®æœ€çµ‚å›ç­”")
                graph_errors_output = gr.HTML(label="å‡¦ç†ã«é–¢ã™ã‚‹é€šçŸ¥ãƒ»ã‚¨ãƒ©ãƒ¼")
            with gr.Column(scale=2):
                graph_output = gr.Image(label="æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³", type="pil", interactive=False, show_download_button=True)

        def submit_fn_wrapper_for_gradio(question, progress=gr.Progress(track_tqdm=True)):
            if not question.strip():
                return ("è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None, "<p style='color:orange;'>è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚</p>")
            
            final_result_tuple = (None, None, None)
            for result_tuple in system.process_question_iterations(question, progress):
                final_result_tuple = result_tuple
            return final_result_tuple

        submit_button.click(fn=submit_fn_wrapper_for_gradio, inputs=[question_input], outputs=[answer_output, graph_output, graph_errors_output])
        gr.Examples(examples=[["æ—¥æœ¬ã®AIæŠ€è¡“ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãã®ç¤¾ä¼šã¸ã®å½±éŸ¿ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚"],["å¤ªé™½å…‰ç™ºé›»ã®ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’æ•´ç†ã—ã€ä»Šå¾Œã®å±•æœ›ã‚’äºˆæ¸¬ã—ã¦ãã ã•ã„ã€‚"]], inputs=[question_input], label="è³ªå•ä¾‹")
        gr.Markdown("---")
        gr.Markdown("Â© 2025 ãƒ¦ãƒ‹ãƒ¼ã‚¯AIã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰é“å ´ G9 (ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— - Geminiç‰ˆ). å›ç­”ç”Ÿæˆã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
    return demo

if __name__ == "__main__":
    if not os.getenv("GEMINI_API_KEY"):
        print("CRITICAL: GEMINI_API_KEY environment variable not set. Please set it to run the application.")

    ai_system_instance = AISystem()
    gradio_app_interface = create_gradio_interface(ai_system_instance)
    gradio_app_interface.launch(server_name="localhost", server_port=7860)