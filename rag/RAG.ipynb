{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx76qjaGKwIS"
      },
      "source": [
        "# æ—¢å­˜è³‡æ–™ã‹ã‚‰ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’ä½œæˆã™ã‚‹(Retrieval Augumented Generation:RAG)\n",
        "\n",
        "- å‚è€ƒ\n",
        "  - https://colab.research.google.com/github/nyanta012/demo/blob/main/sentence_retrieval.ipynb\n",
        "  - https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html\n",
        "  - https://note.com/mahlab/n/nb6677d0fc7c2\n",
        "- ChromadbãŒã™ã”ã„\n",
        "  - https://www.trychroma.com/\n",
        "  - https://github.com/chroma-core/chroma\n",
        "- ChatGPTã®Surveyè«–æ–‡\n",
        "  - https://arxiv.org/pdf/2304.01852.pdf\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pmpfw-x7Kt7H"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "%pip install -U langchain-community\n",
        "%pip install openai\n",
        "# æ®‹ã‚Šã®ã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«è¿½åŠ \n",
        "%pip install -U chromadb langchain\n",
        "%pip install pypdf\n",
        "%pip install tiktoken\n",
        "%pip install gradio\n",
        "%pip install pypdf2\n",
        "%pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fC7q6QnDPIW2"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import VectorDBQA, RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "import openai\n",
        "import os\n",
        "# langchain-openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zdTC8km-OpiG"
      },
      "outputs": [],
      "source": [
        "API_KEY=\"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SZEvBNCdPxmp"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = API_KEY\n",
        "openai.api_key = API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1RVC0SOUL1Dp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "\n",
        "class Document:\n",
        "    def __init__(self, text, metadata):\n",
        "        self.page_content = text\n",
        "        self.metadata = metadata\n",
        "\n",
        "dir_path = r\"C:\\Users\\Takayama Ryuji\\Desktop\\g9\\loadtext\\loadtext\"\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(dir_path):\n",
        "    file_path = os.path.join(dir_path, filename)\n",
        "    metadata = {'filename': filename}\n",
        "\n",
        "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    if filename.endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            text = ''\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "            documents.append(Document(text, metadata))\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    elif filename.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            documents.append(Document(text, metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4azKtNLMb6Kc"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duuXVCZlwPst",
        "outputId": "32c5fa8d-7a1c-43f8-916e-6f2562a3c01e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚µãƒ¼Gilbert Strangã®YouTubeè¬›åº§ã§å­¦ç¿’ã—ãŸç·šå½¢ä»£æ•°ã®æˆæ¥­ã€Œ1. The Geometry of Linear Equationsã€ã®ä¸»è¦ãªè¦ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
            "2. æ•™ç§‘æ›¸p.62ã€œp.106ãƒšãƒ¼ã‚¸ã§å­¦ç¿’ã—ãŸè¡Œåˆ—ã«ã¤ã„ã¦ã®ä¸»è¦ãªæ¦‚å¿µã¨è¦ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
            "3. å˜ç´”ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ãŒğ’š=ğ’˜$ğ’™+ğ‘ã‚’è¡¨ã™ã“ã¨ãŒã§ãã‚‹ã¨ã„ã†äº‹å®Ÿã®æ„å‘³ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
            "4. ãƒã‚¤ã‚¢ã‚¹é …(+ğ‘)ãŒãªã„è¡¨ç¾ã€ã™ãªã‚ã¡ğ’š=ğ’˜$ğ’™ã‚’ã©ã®ã‚ˆã†ã«å¼è¡¨ç¾ã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ\n",
            "5. ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã€ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•è·é›¢ã‚’æ•´ç†ã—ãŸä¸Šã§ãƒãƒãƒ©ãƒãƒ“ã‚¹è·é›¢ã‚’ã©ã®ã‚ˆã†ã«èª¬æ˜ã—ã¾ã™ã‹ï¼Ÿ\n",
            "6. è¡Œåˆ—ã®å†™åƒã®åˆæˆã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
            "7. ç·šå½¢å¤‰æ›fã‚„gã®åˆæˆå†™åƒã‚’ã©ã®ã‚ˆã†ã«ã—ã¦è¡¨ç¾ã—ã¾ã™ã‹ï¼Ÿ\n",
            "8. ç·šå½¢å†™åƒã®æ ¸(Kerf)ã¨ãã®æ„ç¾©ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n",
            "9. scipy.linalg.null_space(A)ã‚’ä½¿ç”¨ã—ã¦Kerfã®åŸºåº•ã‚„ãã®æ¬¡å…ƒã‚’ã©ã®ã‚ˆã†ã«è¨ˆç®—ã—ã¾ã™ã‹ï¼Ÿ\n",
            "10. ç·šå½¢ä»£æ•°ã®å­¦ç¿’ã‚’é€šã˜ã¦ä½•ã‚‰ã‹ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹éš›ã«ä½•ã‚‰ã‹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚„è¡Œåˆ—ã‚’ã©ã®ã‚ˆã†ã«ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ\n"
          ]
        }
      ],
      "source": [
        "# è³ªå•ã‚’ä½œæˆã™ã‚‹ãŸã‚ã« invoke ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨\n",
        "questions = qa.invoke(\"ã“ã®æ–‡ç« ã®ä¸­ã§é‡è¦ã¨æ€ã†ç·šå½¢ä»£æ•°ã«é–¢ã™ã‚‹å†…å®¹ã«ã¤ã„ã¦ã‚’è€ƒãˆã‚‹è³ªå•ã‚’10å€‹è€ƒãˆã¦ãã ã•ã„\")\n",
        "\n",
        "# çµæœã®è¡¨ç¤º\n",
        "print(questions['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuiXiS92vWju",
        "outputId": "4158a9a3-293b-459c-9b7a-f73d53741500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€æä¾›ã•ã‚ŒãŸæ–‡è„ˆå†…ã«ã¯ç–‘ä¼¼é€†è¡Œåˆ—ã«ã¤ã„ã¦ã®æƒ…å ±ã¯å«ã¾ã‚Œã¦ãŠã‚‰ãšã€ãã®ãŸã‚è©³ç´°ã«ã¤ã„ã¦è§£èª¬ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚\n"
          ]
        }
      ],
      "source": [
        "# è³ªå•ã‚’ä½œæˆã™ã‚‹ãŸã‚ã« invoke ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨\n",
        "questions = qa.invoke(\"ç–‘ä¼¼é€†è¡Œåˆ—ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿæ¤œç´¢ã§ããªã‘ã‚Œã°ã‚ã‹ã‚‰ãªã„ã¨ç­”ãˆã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "# çµæœã®è¡¨ç¤º\n",
        "print(questions['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "wh0jw-neN2kc",
        "outputId": "873ea5fa-4254-4c4b-f17f-598134f05af0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Takayama Ryuji\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://ecab908476544743ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ecab908476544743ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import VectorDBQA, RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "dir_path = r\"C:\\Users\\Takayama Ryuji\\Desktop\\g9\\loadtext\\loadtext\"\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(dir_path):\n",
        "    file_path = os.path.join(dir_path, filename)\n",
        "    metadata = {'filename': filename}\n",
        "\n",
        "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    if filename.endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            text = ''\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "            documents.append(Document(text, metadata))\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    elif filename.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            documents.append(Document(text, metadata))\n",
        "\n",
        "\n",
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²ã¨ãƒ™ã‚¯ãƒˆãƒ«DBã®ä½œæˆ\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "# QAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
        "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())\n",
        "\n",
        "# ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…\n",
        "def chat_with_ai(input_text):\n",
        "    response = qa.invoke(input_text+'æ¤œç´¢ã§ããŸç¯„å›²ã§ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢ã§ããªã„ã‚‚ã®ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚')\n",
        "    return response['result']\n",
        "\n",
        "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®è¨­å®š\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_ai,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    title=\"AI Chat with Gradio\",\n",
        "    description=\"OpenAIã‚’ä½¿ã£ãŸãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹\"\n",
        ")\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "collapsed": true,
        "id": "1DBYaPehIXn4",
        "outputId": "3edffb02-9af0-4715-b364-c279fe41b327"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Takayama Ryuji\\AppData\\Local\\Temp\\ipykernel_22460\\4049497658.py:58: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.ChatInterface(fn=chat_with_ai, chatbot=gr.Chatbot())\n",
            "c:\\Users\\Takayama Ryuji\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "* Running on public URL: https://636aa1e776e4a9df57.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://636aa1e776e4a9df57.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Takayama Ryuji\\AppData\\Local\\Temp\\ipykernel_22460\\4049497658.py:53: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = qa({\"query\": input_text+'æ¤œç´¢ã§ããŸç¯„å›²ã§ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢ã§ããªã„ã‚‚ã®ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚'})\n"
          ]
        }
      ],
      "source": [
        "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import VectorDBQA, RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "import gradio as gr\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
        "class Document:\n",
        "    def __init__(self, text, metadata):\n",
        "        self.page_content = text\n",
        "        self.metadata = metadata\n",
        "\n",
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹ã¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ï¼‰\n",
        "dir_path = r\"C:\\Users\\Takayama Ryuji\\Desktop\\g9\\loadtext\\loadtext\"\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in os.listdir(dir_path):\n",
        "    file_path = os.path.join(dir_path, filename)\n",
        "    metadata = {'filename': filename}\n",
        "\n",
        "    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    if filename.endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            text = ''\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "            documents.append(Document(text, metadata))\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
        "    elif filename.endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            documents.append(Document(text, metadata))\n",
        "\n",
        "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²ã¨ãƒ™ã‚¯ãƒˆãƒ«DBã®ä½œæˆ\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(texts, embeddings)\n",
        "\n",
        "# QAãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ\n",
        "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())\n",
        "\n",
        "# ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®å®Ÿè£…\n",
        "def chat_with_ai(input_text, history):\n",
        "    response = qa({\"query\": input_text+'æ¤œç´¢ã§ããŸç¯„å›²ã§ç­”ãˆã¦ãã ã•ã„ã€‚æ¤œç´¢ã§ããªã„ã‚‚ã®ã¯ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚'})\n",
        "    return response['result']\n",
        "\n",
        "# Gradioã®ChatInterfaceã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.ChatInterface(fn=chat_with_ai, chatbot=gr.Chatbot())\n",
        "    gr.Markdown(\"### AI Chat with Gradio\")\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®èµ·å‹•\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
