# type: ignore
import json
from typing import List, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableSequence
from enum import Enum
from dataclasses import dataclass, field
import logging
from functools import lru_cache
import hashlib
from datetime import datetime
import numpy as np

from models import Document
import config

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logger = logging.getLogger(__name__)


class XAIMethod(str, Enum):
    """XAIæ‰‹æ³•ã®åˆ—æŒ™å‹"""
    FEATURE_IMPORTANCE = "ç‰¹å¾´é‡è¦åº¦åˆ†æ"
    COUNTERFACTUAL = "åäº‹å®Ÿèª¬æ˜"
    RULE_BASED = "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜"
    CASE_BASED = "äº‹ä¾‹ãƒ™ãƒ¼ã‚¹èª¬æ˜"
    PROCESS_TRACKING = "ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡"
    ATTENTION_VISUALIZATION = "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–"
    LOCAL_APPROXIMATION = "å±€æ‰€çš„è¿‘ä¼¼"
    MULTI_FACETED = "å¤šè§’çš„èª¬æ˜"
    QWEN3_THINKING = "Qwen3é¢¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹"


@dataclass
class MethodSelection:
    """æ‰‹æ³•é¸æŠã®çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    primary_method: str
    secondary_method: Optional[str] = None
    reasoning: str = ""
    approach: str = ""
    confidence: float = 0.5


class XAIMethodSelector:
    """XAIæ‰‹æ³•é¸æŠã¨èª¬æ˜ç”Ÿæˆã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰"""

    def __init__(self, model_name: str = config.DEFAULT_MODEL_NAME):
        """
        åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰

        Parameters:
        -----------
        model_name : str
            XAIæ‰‹æ³•é¸æŠã«ä½¿ç”¨ã™ã‚‹LLMã®ãƒ¢ãƒ‡ãƒ«å
        """
        self.model_name = model_name
        try:
            self.llm = ChatOpenAI(model_name=model_name)
        except Exception as e:
            logger.error(f"ChatOpenAI ãƒ¢ãƒ‡ãƒ« '{model_name}' ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise

        # XAIæ‰‹æ³•é¸æŠãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ—¢å­˜ã®ã‚‚ã®ã‚’ä¿æŒï¼‰
        # selection_promptã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«æ›´æ–°
        self.selection_prompt = PromptTemplate(
            input_variables=["question", "answer", "domain"],
            template="""
            ã‚ãªãŸã¯æœ€é©ãªXAI(Explainable AI)æ‰‹æ³•ã‚’é¸æŠã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå¿œç­”ã«å¯¾ã—ã¦æœ€ã‚‚é©åˆ‡ãªèª¬æ˜æ‰‹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}
            ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}

            ä»¥ä¸‹ã®èª¬æ˜æ‰‹æ³•ã‹ã‚‰ã€ã“ã®çŠ¶æ³ã«æœ€é©ãªæ‰‹æ³•ã¨ãã®ç†ç”±ã‚’é¸æŠã—ã¦ãã ã•ã„:
            
            ã€åŸºæœ¬çš„ãªèª¬æ˜æ‰‹æ³•ã€‘
            1. ç‰¹å¾´é‡è¦åº¦åˆ†æ - å›ç­”ã«å½±éŸ¿ã‚’ä¸ãˆãŸå…¥åŠ›ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–
            2. åäº‹å®Ÿèª¬æ˜ - ã€Œã‚‚ã—ã€œãŒç•°ãªã£ã¦ã„ãŸã‚‰ã€ã¨ã„ã†ä»®å®šã§ã®èª¬æ˜
            3. ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜ - if-thenãƒ«ãƒ¼ãƒ«ã¨ã—ã¦è¡¨ç¾
            4. äº‹ä¾‹ãƒ™ãƒ¼ã‚¹èª¬æ˜ - é¡ä¼¼äº‹ä¾‹ã¨æ¯”è¼ƒã—ã¦èª¬æ˜
            5. ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡ - ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–éç¨‹ã‚’æ®µéšçš„ã«èª¬æ˜
            
            ã€é«˜åº¦ãªèª¬æ˜æ‰‹æ³•ã€‘
            6. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ– - ãƒ¢ãƒ‡ãƒ«ãŒæ³¨ç›®ã—ãŸæƒ…å ±ã‚’å¼·èª¿
            7. å±€æ‰€çš„è¿‘ä¼¼ - è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã‚’å±€æ‰€çš„ã«å˜ç´”åŒ–
            8. å¤šè§’çš„èª¬æ˜ - è¤‡æ•°ã®èª¬æ˜æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›
            9. Qwen3é¢¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ - è‡ªç„¶ãªæ€è€ƒã®æµã‚Œã‚’è¡¨ç¾
            
            ã€æ–°ã—ã„èª¬æ˜æ‰‹æ³•ã€‘
            10. æ€è€ƒã®é€£é– - äººé–“ãŒç†è§£ã—ã‚„ã™ã„æ®µéšçš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹
            11. æ¯”è¼ƒåˆ†æ - è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰ä»£æ›¿æ¡ˆã¨æ¯”è¼ƒ
            12. ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ– - ç¢ºä¿¡åº¦ã‚’å®šé‡çš„ã«è©•ä¾¡ã—å¯è¦–åŒ–

            é¸æŠã®éš›ã¯ä»¥ä¸‹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ï¼š
            - è³ªå•ã®è¤‡é›‘ã•ã¨ç¨®é¡
            - æ±‚ã‚ã‚‰ã‚Œã‚‹èª¬æ˜ã®è©³ç´°åº¦
            - ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹æ€§
            - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ€è¡“çš„èƒŒæ™¯ï¼ˆæ¨å®šï¼‰

            å›ç­”å½¢å¼:
            {{
            "primary_method": "é¸æŠã—ãŸä¸»è¦æ‰‹æ³•",
            "secondary_method": "è£œåŠ©çš„æ‰‹æ³•ï¼ˆå¿…è¦ãªå ´åˆï¼‰",
            "reasoning": "é¸æŠç†ç”±",
            "approach": "å…·ä½“çš„ãªå®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
            "confidence": 0.0-1.0ã®ç¢ºä¿¡åº¦
            }}

            JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
            """
        )

        # æ—¢å­˜ã®å®Ÿè£…ã‚’ä¿æŒ
        self.method_implementations = {
            "ç‰¹å¾´é‡è¦åº¦åˆ†æ": self._generate_feature_importance_explanation,
            "åäº‹å®Ÿèª¬æ˜": self._generate_counterfactual_explanation,
            "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜": self._generate_rule_based_explanation,
            "äº‹ä¾‹ãƒ™ãƒ¼ã‚¹èª¬æ˜": self._generate_case_based_explanation,
            "ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡": self._generate_process_tracking_explanation,
            "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–": self._generate_attention_visualization_explanation,
            "å±€æ‰€çš„è¿‘ä¼¼": self._generate_local_approximation_explanation,
            "å¤šè§’çš„èª¬æ˜": self._generate_multi_faceted_explanation,
            "Qwen3é¢¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹": self._generate_qwen3_thinking_process_explanation,
            "æ€è€ƒã®é€£é–": self._generate_chain_of_thought_explanation,
            "æ¯”è¼ƒåˆ†æ": self._generate_comparative_analysis_explanation,
            "ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–": self._generate_uncertainty_quantification_explanation,
        }
        
        self._explanation_cache = {}
        self._cache_max_size = 100  # æœ€å¤§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
        
    @lru_cache(maxsize=32)
    def _get_cache_key(self, question: str, answer: str, method: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        content = f"{method}:{question}:{answer}"
        return hashlib.md5(content.encode()).hexdigest()

    def generate_explanation(self, method: str, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã«åŸºã¥ã„ã¦èª¬æ˜ã‚’ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿèƒ½ä»˜ãï¼‰"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆã‚½ãƒ¼ã‚¹ã¯å«ã‚ãªã„ - é »ç¹ã«å¤‰ã‚ã‚‹ãŸã‚ï¼‰
        use_cache = kwargs.get('use_cache', True)
        if use_cache:
            cache_key = self._get_cache_key(question, answer, method)
            if cache_key in self._explanation_cache:
                logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª¬æ˜ã‚’å–å¾—: {method}")
                return self._explanation_cache[cache_key]
        
        try:
            # èª¬æ˜ã‚’ç”Ÿæˆ
            if method in self.method_implementations:
                explanation = self.method_implementations[method](question, answer, sources, **kwargs)
            else:
                logger.warning(f"ä¸æ˜ãªXAIæ‰‹æ³• '{method}' ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚")
                explanation = self._generate_qwen3_thinking_process_explanation(question, answer, sources, **kwargs)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if use_cache and len(self._explanation_cache) < self._cache_max_size:
                self._explanation_cache[cache_key] = explanation
                logger.info(f"èª¬æ˜ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {method}")
            elif use_cache and len(self._explanation_cache) >= self._cache_max_size:
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ï¼ˆç°¡æ˜“çš„ãªLRUï¼‰
                oldest_key = next(iter(self._explanation_cache))
                del self._explanation_cache[oldest_key]
                self._explanation_cache[cache_key] = explanation
            
            return explanation
            
        except Exception as e:
            logger.error(f"'{method}' èª¬æ˜ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return f"èª¬æ˜ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._explanation_cache.clear()
        self._get_cache_key.cache_clear()
        logger.info("èª¬æ˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")


    def _generate_chain_of_thought_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """æ€è€ƒã®é€£é–ã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯æ®µéšçš„æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€äººé–“ãŒç†è§£ã—ã‚„ã™ã„æ®µéšçš„ãªæ€è€ƒã®é€£é–ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ## æ€è€ƒã®é€£é– (Chain of Thought)

            ### ğŸ¯ ã‚¹ãƒ†ãƒƒãƒ— 1: å•é¡Œã®ç†è§£ã¨åˆ†è§£
            **è³ªå•ã®æ ¸å¿ƒ**: 
            - ä¸»è¦ãªå•ã„ã‹ã‘: ...
            - æš—é»™ã®å‰æ: ...
            - æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã®ç¨®é¡: ...

            ### ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ— 2: åˆ©ç”¨å¯èƒ½ãªæƒ…å ±ã®æ•´ç†
            **æƒ…å ±æºã®åˆ†é¡**:
            - ç›´æ¥é–¢é€£ã™ã‚‹æƒ…å ±: ...
            - è£œåŠ©çš„ãªæƒ…å ±: ...
            - èƒŒæ™¯çŸ¥è­˜: ...

            ### ğŸ” ã‚¹ãƒ†ãƒƒãƒ— 3: æ®µéšçš„æ¨è«–
            **æ¨è«–ã®æµã‚Œ**:
            1. ã¾ãšã€... ã ã‹ã‚‰ ...
            2. æ¬¡ã«ã€... ã—ãŸãŒã£ã¦ ...
            3. ã•ã‚‰ã«ã€... ã‚†ãˆã« ...
            4. æœ€å¾Œã«ã€... ã‚ˆã£ã¦ ...

            ### âœ… ã‚¹ãƒ†ãƒƒãƒ— 4: çµè«–ã®å°å‡º
            **æœ€çµ‚çš„ãªç­”ãˆ**:
            - ä¸»è¦ãªçµè«–: ...
            - è£œè¶³æƒ…å ±: ...
            - æ³¨æ„äº‹é …: ...

            ### ğŸ”„ ã‚¹ãƒ†ãƒƒãƒ— 5: è‡ªå·±æ¤œè¨¼
            **è«–ç†çš„æ•´åˆæ€§ã®ãƒã‚§ãƒƒã‚¯**:
            - âœ“ å‰æã¨çµè«–ã®æ•´åˆæ€§: ...
            - âœ“ æ¨è«–ã®å¦¥å½“æ€§: ...
            - âœ“ æƒ…å ±æºã¨ã®ä¸€è‡´: ...
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_comparative_analysis_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """æ¯”è¼ƒåˆ†æã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯æ¯”è¼ƒåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€è¤‡æ•°ã®è¦³ç‚¹ã‹ã‚‰æ¯”è¼ƒåˆ†æã‚’è¡Œã„ã€ãªãœã“ã®å›ç­”ãŒé¸ã°ã‚ŒãŸã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ## ğŸ“Š æ¯”è¼ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

            ### 1. å›ç­”ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¯”è¼ƒ
            
            | ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | å†…å®¹ | é•·æ‰€ | çŸ­æ‰€ | æ¡ç”¨ç†ç”± |
            |-----------|------|------|------|----------|
            | **æ¡ç”¨æ¡ˆ** | {answer} | ... | ... | âœ… é¸æŠ |
            | ä»£æ›¿æ¡ˆ1 | ... | ... | ... | âŒ ä¸æ¡ç”¨ |
            | ä»£æ›¿æ¡ˆ2 | ... | ... | ... | âŒ ä¸æ¡ç”¨ |

            ### 2. æƒ…å ±æºã®æ¯”è¼ƒè©•ä¾¡
            
            **æƒ…å ±æºã®ä¿¡é ¼æ€§ãƒãƒˆãƒªãƒƒã‚¯ã‚¹**:
            - æƒ…å ±æºA: ä¿¡é ¼åº¦ [â– â– â– â– â–¡] 80% - ç†ç”±: ...
            - æƒ…å ±æºB: ä¿¡é ¼åº¦ [â– â– â– â–¡â–¡] 60% - ç†ç”±: ...
            - æƒ…å ±æºC: ä¿¡é ¼åº¦ [â– â– â–¡â–¡â–¡] 40% - ç†ç”±: ...

            ### 3. è¦–ç‚¹åˆ¥ã®æ¯”è¼ƒ
            
            **æŠ€è¡“çš„è¦–ç‚¹**: 
            - ç¾åœ¨ã®å›ç­”: ...
            - åˆ¥ã®è¦–ç‚¹: ...
            
            **å®Ÿç”¨çš„è¦–ç‚¹**:
            - ç¾åœ¨ã®å›ç­”: ...
            - åˆ¥ã®è¦–ç‚¹: ...
            
            **ç†è«–çš„è¦–ç‚¹**:
            - ç¾åœ¨ã®å›ç­”: ...
            - åˆ¥ã®è¦–ç‚¹: ...

            ### 4. ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ
            
            **ç²¾åº¦ vs ç†è§£ã—ã‚„ã™ã•**:
            - ç¾åœ¨ã®é¸æŠ: ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆç²¾åº¦70%, ç†è§£åº¦80%ï¼‰
            - ä»£æ›¿é¸æŠA: ç²¾åº¦é‡è¦–ï¼ˆç²¾åº¦90%, ç†è§£åº¦50%ï¼‰
            - ä»£æ›¿é¸æŠB: ç†è§£é‡è¦–ï¼ˆç²¾åº¦50%, ç†è§£åº¦95%ï¼‰

            ### 5. æœ€çµ‚è©•ä¾¡
            **ãªãœã“ã®å›ç­”ãŒæœ€é©ã‹**:
            - ä¸»è¦ãªå¼·ã¿: ...
            - å—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹ãƒªã‚¹ã‚¯: ...
            - ç·åˆè©•ä¾¡: â­â­â­â­â˜†
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_uncertainty_quantification_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ã«ã‚ˆã‚‹èª¬æ˜ç”Ÿæˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯ä¸ç¢ºå®Ÿæ€§åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€å„è¦ç´ ã®ç¢ºå®Ÿæ€§ãƒ»ä¸ç¢ºå®Ÿæ€§ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ## ğŸ¯ ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–åˆ†æ

            ### 1. çŸ¥è­˜ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆã‚¨ãƒ”ã‚¹ãƒ†ãƒŸãƒƒã‚¯ä¸ç¢ºå®Ÿæ€§ï¼‰
            
            **æƒ…å ±ã®å®Œå…¨æ€§è©•ä¾¡**:
            ```
            å®Œå…¨æ€§ã‚¹ã‚³ã‚¢: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] 82/100
            - âœ… ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹é ˜åŸŸ: ...
            - âš ï¸ ä¸è¶³ã—ã¦ã„ã‚‹æƒ…å ±: ...
            - âŒ æœªçŸ¥ã®é ˜åŸŸ: ...
            ```

            **æƒ…å ±æºã®ä¿¡é ¼æ€§**:
            ```
            ç·åˆä¿¡é ¼åº¦: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 75/100
            - ä¸€æ¬¡æƒ…å ±æº: 90% ä¿¡é ¼åº¦
            - äºŒæ¬¡æƒ…å ±æº: 70% ä¿¡é ¼åº¦  
            - æ¨è«–éƒ¨åˆ†: 60% ä¿¡é ¼åº¦
            ```

            ### 2. å¶ç„¶ã®ä¸ç¢ºå®Ÿæ€§ï¼ˆã‚¢ãƒ¬ã‚¢ãƒˆãƒªãƒƒã‚¯ä¸ç¢ºå®Ÿæ€§ï¼‰
            
            **å¤‰å‹•è¦å› ã®åˆ†æ**:
            - æ–‡è„ˆä¾å­˜æ€§: ä¸­ç¨‹åº¦ï¼ˆçŠ¶æ³ã«ã‚ˆã‚Šè§£é‡ˆãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ï¼‰
            - æ™‚é–“çš„å¤‰å‹•: ä½ï¼ˆæƒ…å ±ã®æ™‚é–“çš„å®‰å®šæ€§ã¯é«˜ã„ï¼‰
            - å€‹äººå·®: é«˜ï¼ˆèª­ã¿æ‰‹ã«ã‚ˆã‚Šç†è§£ãŒç•°ãªã‚‹å¯èƒ½æ€§ï¼‰

            ### 3. ç¢ºä¿¡åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            
            å›ç­”ã®å„éƒ¨åˆ†ã®ç¢ºä¿¡åº¦ã‚’è¦–è¦šåŒ–ï¼š
            
            ```
            [å›ç­”ã®ç¬¬1æ–‡] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 95% - æƒ…å ±æºã‹ã‚‰ç›´æ¥ç¢ºèª
            [å›ç­”ã®ç¬¬2æ–‡] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 85% - è¤‡æ•°æºã‹ã‚‰æ¨è«–
            [å›ç­”ã®ç¬¬3æ–‡] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 65% - å˜ä¸€æºã‹ã‚‰ã®æ¨è«–
            [å›ç­”ã®ç¬¬4æ–‡] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 45% - ä¸€èˆ¬çš„çŸ¥è­˜ã‹ã‚‰ã®æ¨è«–
            ```

            ### 4. ãƒªã‚¹ã‚¯è©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
            
            | ãƒªã‚¹ã‚¯è¦å›  | ç™ºç”Ÿç¢ºç‡ | å½±éŸ¿åº¦ | ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ« |
            |-----------|---------|--------|-------------|
            | æƒ…å ±ã®èª¤è§£é‡ˆ | ä½ (20%) | ä¸­ | ğŸŸ¡ ä½ãƒªã‚¹ã‚¯ |
            | æ–‡è„ˆã®è¦‹è½ã¨ã— | ä¸­ (40%) | é«˜ | ğŸŸ  ä¸­ãƒªã‚¹ã‚¯ |
            | æ¨è«–ã®èª¤ã‚Š | ä½ (15%) | é«˜ | ğŸŸ¡ ä½ãƒªã‚¹ã‚¯ |
            | æƒ…å ±ã®é™³è…åŒ– | ä½ (10%) | ä½ | ğŸŸ¢ æ¥µä½ãƒªã‚¹ã‚¯ |

            ### 5. ä¿¡é ¼åŒºé–“ã¨æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            
            **å›ç­”ã®ä¿¡é ¼åŒºé–“**: 70% - 90%
            - æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„è§£é‡ˆ: {answer}
            - ä¸‹é™ã®è§£é‡ˆ: ã‚ˆã‚Šä¿å®ˆçš„ãªè¦‹è§£...
            - ä¸Šé™ã®è§£é‡ˆ: ã‚ˆã‚Šç©æ¥µçš„ãªè¦‹è§£...

            **æ¨å¥¨ã•ã‚Œã‚‹è¿½åŠ ç¢ºèª**:
            1. ğŸ” è¿½åŠ ã§ç¢ºèªã™ã¹ãæƒ…å ±æº: ...
            2. ğŸ’¡ å°‚é–€å®¶ã¸ã®ç›¸è«‡ãŒæ¨å¥¨ã•ã‚Œã‚‹éƒ¨åˆ†: ...
            3. â° å®šæœŸçš„ãªè¦‹ç›´ã—ãŒå¿…è¦ãªè¦ç´ : ...

            ### 6. ç·åˆçš„ãªç¢ºå®Ÿæ€§è©•ä¾¡

            **å…¨ä½“çš„ãªç¢ºä¿¡åº¦**: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘] 72%

            ã“ã®è©•ä¾¡ã¯ä»¥ä¸‹ã®è¦å› ã«åŸºã¥ã„ã¦ã„ã¾ã™ï¼š
            - æƒ…å ±æºã®è³ªã¨é‡
            - æ¨è«–ã®è«–ç†çš„å¦¥å½“æ€§
            - æ—¢çŸ¥ã®åˆ¶é™äº‹é …
            - æ½œåœ¨çš„ãªãƒã‚¤ã‚¢ã‚¹
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)


    def select_methods(self, question: str, answer: str, domain: str = "ä¸€èˆ¬") -> Dict[str, Any]:
        """
        æœ€é©ãªXAIæ‰‹æ³•ã‚’é¸æŠ

        Parameters:
        -----------
        question : str
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        answer : str
            AIã®å›ç­”
        domain : str, optional
            ãƒ‰ãƒ¡ã‚¤ãƒ³æƒ…å ±

        Returns:
        --------
        dict
            é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã¨ç†ç”±
        """
        try:
            # RunnableSequenceã‚’ä½¿ç”¨ï¼ˆLangChain v0.3+æ¨å¥¨ï¼‰
            chain = self.selection_prompt | self.llm

            response = chain.invoke({
                "question": question,
                "answer": answer,
                "domain": domain
            })

            # responseãŒAIMessageã®å ´åˆã€contentã‚’å–å¾—
            response_text = response.content if hasattr(response, 'content') else str(response)

            # æ–‡å­—åˆ—ã‹ã‚‰JSONã‚’æŠ½å‡º
            json_str = response_text
            if "{" in json_str and "}" in json_str:
                start = json_str.find("{")
                end = json_str.rfind("}") + 1
                json_str = json_str[start:end]

            result = json.loads(json_str)

            # çµæœã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            if "primary_method" not in result:
                raise ValueError("primary_methodãŒçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

            return result

        except json.JSONDecodeError as e:
            logger.error(f"XAIæ‰‹æ³•é¸æŠçµæœã®JSONãƒ‘ãƒ¼ã‚¹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logger.error(f"å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text if 'response_text' in locals() else 'N/A'}")
            return self._default_method_selection("JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼")
        except Exception as e:
            logger.error(f"XAIæ‰‹æ³•é¸æŠä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return self._default_method_selection(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")

    def _default_method_selection(self, reason: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®XAIæ‰‹æ³•é¸æŠçµæœã‚’è¿”ã™"""
        return {
            "primary_method": "Qwen3é¢¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹",
            "secondary_method": "ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡",
            "reasoning": f"ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ‰‹æ³•ã‚’é¸æŠã—ã¾ã—ãŸ ({reason})ã€‚",
            "approach": "åŸºæœ¬çš„ãªèª¬æ˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é©ç”¨ã—ã¾ã™ã€‚"
        }

    def generate_explanation(self, method: str, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """
        é¸æŠã•ã‚ŒãŸæ‰‹æ³•ã«åŸºã¥ã„ã¦èª¬æ˜ã‚’ç”Ÿæˆ

        Parameters:
        -----------
        method : str
            ä½¿ç”¨ã™ã‚‹XAIæ‰‹æ³•
        question : str
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
        answer : str
            AIã®å›ç­”
        sources : list[Document]
            é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        **kwargs : dict
            ãã®ä»–ã®å¼•æ•°

        Returns:
        --------
        str
            ç”Ÿæˆã•ã‚ŒãŸèª¬æ˜
        """
        try:
            if method in self.method_implementations:
                return self.method_implementations[method](question, answer, sources, **kwargs)
            else:
                logger.warning(f"ä¸æ˜ãªXAIæ‰‹æ³• '{method}' ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Qwen3é¢¨æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return self._generate_qwen3_thinking_process_explanation(question, answer, sources, **kwargs)
        except Exception as e:
            logger.error(f"'{method}' èª¬æ˜ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return f"èª¬æ˜ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®èª¬æ˜ã‚’æä¾›ã—ã¾ã™ã€‚\n\n" + \
                self._generate_qwen3_thinking_process_explanation(question, answer, sources, **kwargs)

    def _format_sources_text(self, sources: List[Document]) -> str:
        """ã‚½ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‚’æ•´å½¢ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
        sources_text = ""
        if not sources:
            return "åˆ©ç”¨å¯èƒ½ãªæƒ…å ±æºã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n\n"

        for i, source in enumerate(sources):
            filename = source.metadata.get('filename', 'Unknown')
            content = source.page_content
            truncated_content = (content[:500] + '...') if len(content) > 500 else content
            sources_text += f"[æƒ…å ±æº {i+1}] {filename}:\n"
            sources_text += f"{truncated_content}\n\n"
        return sources_text

    def _run_llm_chain(self, prompt: PromptTemplate, inputs: Dict[str, Any]) -> str:
        """LLMãƒã‚§ãƒ¼ãƒ³ã‚’å®Ÿè¡Œã—ã€çµæœã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
        try:
            # RunnableSequenceã‚’ä½¿ç”¨
            chain = prompt | self.llm
            response = chain.invoke(inputs)
            
            # responseãŒAIMessageã®å ´åˆã€contentã‚’å–å¾—
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
                
        except Exception as e:
            logger.error(f"LLMãƒã‚§ãƒ¼ãƒ³å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return f"ã‚¨ãƒ©ãƒ¼: èª¬æ˜ç”Ÿæˆä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ ({e})ã€‚"

    # === ä»¥ä¸‹ã€æ—¢å­˜ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãã®ã¾ã¾ä¿æŒ ===
    
    def _generate_qwen3_thinking_process_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """Qwen3é¢¨ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ã‚’å¾—æ„ã¨ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒè€ƒãˆãŸæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’<think>ã‚¿ã‚°ã§å›²ã¿ã€æœ€å¾Œã«æœ€çµ‚å›ç­”ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            Qwen3é¢¨ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¨ã—ã¦ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§èª¬æ˜ã—ã¦ãã ã•ã„:

            <think>
            ã“ã“ã§ã¯ã€å›ç­”ã«è‡³ã‚‹ã¾ã§ã®æ€è€ƒéç¨‹ã‚’è©³ç´°ã«è¨˜è¿°ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ç‚¹ã‚’è‡ªç„¶ãªæ€è€ƒã®æµã‚Œã§èª¬æ˜ã—ã¦ãã ã•ã„:
            1. è³ªå•ã®æ„å›³ã‚„è¦ä»¶ã®ç†è§£
            2. é–¢é€£ã™ã‚‹æƒ…å ±æºã‹ã‚‰ã®çŸ¥è­˜ã®æŠ½å‡º
            3. è¤‡æ•°ã®è¦–ç‚¹ã‹ã‚‰ã®æ¤œè¨
            4. è«–ç†çš„æ¨è«–ã®ã‚¹ãƒ†ãƒƒãƒ—
            5. å›ç­”ã®å¦¥å½“æ€§è©•ä¾¡
            </think>

            æœ€çµ‚å›ç­”: ï¼ˆã“ã“ã«ã¯å…ƒã®å›ç­”ã‚’ãã®ã¾ã¾ã€ã¾ãŸã¯å¿…è¦ã«å¿œã˜ã¦æ”¹å–„ã—ã¦è¨˜è¼‰ï¼‰
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_feature_importance_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """ç‰¹å¾´é‡è¦åº¦åˆ†æã«ã‚ˆã‚‹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯ç‰¹å¾´é‡è¦åº¦åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€ã©ã®å…¥åŠ›æƒ…å ±ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ–‡è„ˆï¼‰ãŒå›ç­”ç”Ÿæˆã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆãŸã‹ã‚’åˆ†æã—ã€èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: è³ªå•å†…ã®é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šã—ã€ãã‚Œã‚‰ãŒå›ç­”ã«ã©ã†å½±éŸ¿ã—ãŸã‹ã‚’èª¬æ˜
            ã‚¹ãƒ†ãƒƒãƒ—2: å¤–éƒ¨çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã®é‡è¦éƒ¨åˆ†ã‚’ç‰¹å®šã—ã€é‡è¦åº¦ã‚’5æ®µéšã§è©•ä¾¡
            ã‚¹ãƒ†ãƒƒãƒ—3: å›ç­”ã®å„ãƒ‘ãƒ¼ãƒˆãŒã©ã®æƒ…å ±ã‚½ãƒ¼ã‚¹ã«ä¾å­˜ã—ã¦ã„ã‚‹ã‹ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_counterfactual_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """åäº‹å®Ÿèª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯åäº‹å®Ÿèª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€å…¥åŠ›ãŒç•°ãªã£ã¦ã„ãŸå ´åˆã«å›ç­”ãŒã©ã†å¤‰ã‚ã£ã¦ã„ãŸã‹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: è³ªå•ã®é‡è¦ãªè¦ç´ ã‚’ç‰¹å®šã—ã€ãã‚Œã‚‰ãŒå¤‰ã‚ã£ãŸå ´åˆã®ä»£æ›¿å›ç­”ã‚’ç¤ºã™
            ã‚¹ãƒ†ãƒƒãƒ—2: ã€Œã‚‚ã—ã€œãŒç•°ãªã£ã¦ã„ãŸã‚‰ã€å›ç­”ã¯ã€œã«ãªã£ã¦ã„ãŸã€ã¨ã„ã†å½¢å¼ã§3ã¤ã®åäº‹å®Ÿã‚·ãƒŠãƒªã‚ªã‚’æç¤º
            ã‚¹ãƒ†ãƒƒãƒ—3: ãªãœãƒ¢ãƒ‡ãƒ«ãŒã“ã®å›ç­”ã‚’é¸æŠã—ã€ä»–ã®å¯èƒ½æ€§ã‚’é™¤å¤–ã—ãŸã®ã‹ã‚’èª¬æ˜
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_rule_based_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹èª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã‚’if-thenãƒ«ãƒ¼ãƒ«ã®å½¢å¼ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã—ãŸã¨æ€ã‚ã‚Œã‚‹æ±ºå®šãƒ«ãƒ¼ãƒ«ã‚’æŠ½å‡º
            ã‚¹ãƒ†ãƒƒãƒ—2: ã€Œã‚‚ã—ã€œãªã‚‰ã°ã€ã€œã¨åˆ¤æ–­ã™ã‚‹ã€ã¨ã„ã†å½¢å¼ã§5-7å€‹ã®ãƒ«ãƒ¼ãƒ«ã‚’æç¤º
            ã‚¹ãƒ†ãƒƒãƒ—3: ã“ã‚Œã‚‰ã®ãƒ«ãƒ¼ãƒ«ãŒå®Ÿéš›ã®å›ç­”ã«ã©ã®ã‚ˆã†ã«é©ç”¨ã•ã‚ŒãŸã‹ã‚’èª¬æ˜
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_case_based_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """äº‹ä¾‹ãƒ™ãƒ¼ã‚¹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯äº‹ä¾‹ãƒ™ãƒ¼ã‚¹èª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€é¡ä¼¼ã™ã‚‹æ—¢çŸ¥ã®äº‹ä¾‹ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: ã“ã®è³ªå•/å›ç­”ã«é¡ä¼¼ã™ã‚‹äº‹ä¾‹ã‚’å‚ç…§æƒ…å ±ã‹ã‚‰ç‰¹å®š
            ã‚¹ãƒ†ãƒƒãƒ—2: å‚ç…§æƒ…å ±ã®äº‹ä¾‹ã¨ã“ã®å›ç­”ã®é¡ä¼¼ç‚¹ã¨ç›¸é•ç‚¹ã‚’åˆ†æ
            ã‚¹ãƒ†ãƒƒãƒ—3: ãªãœãƒ¢ãƒ‡ãƒ«ãŒã“ã®äº‹ä¾‹ã‹ã‚‰å­¦ç¿’ã—ã€é©ç”¨ã—ãŸã®ã‹ã‚’èª¬æ˜
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_process_tracking_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡ã«ã‚ˆã‚‹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’èª¬æ˜ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–éç¨‹ã‚’æ®µéšçš„ã«å†ç¾ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§èª¬æ˜ã—ã¦ãã ã•ã„:

            ## æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è¿½è·¡
            1. **æœ€åˆã®ç†è§£**: ãƒ¢ãƒ‡ãƒ«ã¯ã¾ãšè³ªå•ã‚’ã©ã†è§£é‡ˆã—ãŸã‹
            2. **æƒ…å ±åé›†**: ã©ã®çŸ¥è­˜ã‚½ãƒ¼ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€ä½•ã‚’æŠ½å‡ºã—ãŸã‹
            3. **æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—**: ã©ã®ã‚ˆã†ãªä¸­é–“çš„ãªçµè«–ã‚’å°ã„ãŸã‹
            4. **æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹**: ã©ã®ã‚ˆã†ã«çµè«–ã‚’ç¢ºèªã—ãŸã‹
            5. **æœ€çµ‚åˆ¤æ–­**: ãªãœã“ã®å›ç­”ãŒæœ€é©ã¨åˆ¤æ–­ã—ãŸã‹

            ## ç¢ºä¿¡åº¦åˆ†æ
            å„ã‚¹ãƒ†ãƒƒãƒ—ã®ç¢ºä¿¡åº¦ã¨ä¸ç¢ºå®Ÿæ€§

            ## ä»£æ›¿è§£é‡ˆ
            è€ƒæ…®ã•ã‚ŒãŸãŒæ¡ç”¨ã•ã‚Œãªã‹ã£ãŸä»£æ›¿æ¡ˆ
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_attention_visualization_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–ã«ã‚ˆã‚‹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã«æ³¨ç›®ã—ãŸã‹ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: è³ªå•å†…ã®é‡è¦ãªéƒ¨åˆ†ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã€é‡è¦åº¦ã‚’èª¬æ˜
            ã‚¹ãƒ†ãƒƒãƒ—2: å›ç­”ç”Ÿæˆã«ç‰¹ã«å½±éŸ¿ã‚’ä¸ãˆãŸæƒ…å ±æºã®éƒ¨åˆ†ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã€é‡è¦åº¦ã‚’èª¬æ˜
            ã‚¹ãƒ†ãƒƒãƒ—3: è³ªå•ã¨å›ç­”ã®é–“ã®é–¢é€£æ€§ã‚’ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æµã‚Œã¨ã—ã¦èª¬æ˜

            é‡è¦åº¦ã®è¡¨ç¾ã«ã¯ä»¥ä¸‹ã®è¨˜å·ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:
            [é‡è¦åº¦â˜…â˜…â˜…] éå¸¸ã«é‡è¦ãªæƒ…å ±
            [é‡è¦åº¦â˜…â˜…] ä¸­ç¨‹åº¦ã«é‡è¦ãªæƒ…å ±
            [é‡è¦åº¦â˜…] é–¢é€£æ€§ã®ã‚ã‚‹æƒ…å ±
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_local_approximation_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """å±€æ‰€çš„è¿‘ä¼¼ã«ã‚ˆã‚‹èª¬æ˜ã‚’ç”Ÿæˆ"""
        prompt = PromptTemplate(
            input_variables=["question", "answer", "sources"],
            template="""
            ã‚ãªãŸã¯å±€æ‰€çš„è¿‘ä¼¼èª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®AIå›ç­”ã«ã¤ã„ã¦ã€è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šã‚’å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã§è¿‘ä¼¼ã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            å‚ç…§æƒ…å ±:
            {sources}

            ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šã‚’3-5å€‹ã®ä¸»è¦å› å­ã«å˜ç´”åŒ–ã—ã¦èª¬æ˜
            ã‚¹ãƒ†ãƒƒãƒ—2: å„å› å­ãŒæœ€çµ‚æ±ºå®šã«ã©ã®ç¨‹åº¦å½±éŸ¿ã—ãŸã‹ã®å‰²åˆã‚’æç¤º
            ã‚¹ãƒ†ãƒƒãƒ—3: ã“ã®å˜ç´”åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒå…ƒã®è¤‡é›‘ãªãƒ¢ãƒ‡ãƒ«ã®æ±ºå®šã‚’ã©ã®ç¨‹åº¦æ­£ç¢ºã«è¿‘ä¼¼ã—ã¦ã„ã‚‹ã‹ã‚’èª¬æ˜
            """
        )
        sources_text = self._format_sources_text(sources)
        inputs = {"question": question, "answer": answer, "sources": sources_text}
        return self._run_llm_chain(prompt, inputs)

    def _generate_multi_faceted_explanation(self, question: str, answer: str, sources: List[Document], **kwargs) -> str:
        """å¤šè§’çš„èª¬æ˜ã‚’ç”Ÿæˆ"""
        primary_explanation = ""
        secondary_explanation = ""
        try:
            primary_explanation = self._generate_process_tracking_explanation(question, answer, sources, **kwargs)
        except Exception as e:
            logger.error(f"å¤šè§’çš„èª¬æ˜ã®ãƒ—ãƒ©ã‚¤ãƒãƒªç”Ÿæˆï¼ˆãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡ï¼‰ã§ã‚¨ãƒ©ãƒ¼: {e}")
            primary_explanation = "ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡ã«ã‚ˆã‚‹èª¬æ˜ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

        try:
            secondary_explanation = self._generate_feature_importance_explanation(question, answer, sources, **kwargs)
        except Exception as e:
            logger.error(f"å¤šè§’çš„èª¬æ˜ã®ã‚»ã‚«ãƒ³ãƒ€ãƒªç”Ÿæˆï¼ˆç‰¹å¾´é‡è¦åº¦ï¼‰ã§ã‚¨ãƒ©ãƒ¼: {e}")
            secondary_explanation = "ç‰¹å¾´é‡è¦åº¦ã«ã‚ˆã‚‹èª¬æ˜ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

        prompt = PromptTemplate(
            input_variables=["primary", "secondary", "question", "answer"],
            template="""
            ã‚ãªãŸã¯å¤šè§’çš„AIèª¬æ˜ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®2ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹èª¬æ˜ã‚’çµ±åˆã—ã¦ã€ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªèª¬æ˜ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}
            AIã®å›ç­”: {answer}

            èª¬æ˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1 (ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡):
            {primary}

            èª¬æ˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2 (ç‰¹å¾´é‡è¦åº¦):
            {secondary}

            ã“ã‚Œã‚‰ã®èª¬æ˜ã‚’çµ±åˆã—ã€ä»¥ä¸‹ã®æ§‹é€ ã§åŒ…æ‹¬çš„ãªèª¬æ˜ã‚’æä¾›ã—ã¦ãã ã•ã„:

            ## 1. æ¦‚è¦
            å›ç­”ã®ä¸»è¦ãƒã‚¤ãƒ³ãƒˆã¨æ ¹æ‹ ã®ç°¡æ½”ãªè¦ç´„

            ## 2. æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹
            ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«è€ƒãˆã€çµè«–ã«è‡³ã£ãŸã‹ (ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ1ã«åŸºã¥ã)

            ## 3. é‡è¦ãªå½±éŸ¿è¦å› 
            å›ç­”å½¢æˆã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆãŸæƒ…å ±ã¨è¦ç´  (ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ2ã«åŸºã¥ã)

            ## 4. ä»£æ›¿å¯èƒ½æ€§
            è€ƒæ…®ã•ã‚ŒãŸä»–ã®å¯èƒ½æ€§ã¨ã€ãã‚Œã‚‰ãŒé¸æŠã•ã‚Œãªã‹ã£ãŸç†ç”± (å¯èƒ½ã§ã‚ã‚Œã°è¨€åŠ)

            ## 5. ä¿¡é ¼æ€§è©•ä¾¡
            ã“ã®å›ç­”ã®ç¢ºå®Ÿæ€§ã¨åˆ¶é™äº‹é … (å¯èƒ½ã§ã‚ã‚Œã°è¨€åŠ)
            """
        )

        inputs = {
            "primary": primary_explanation,
            "secondary": secondary_explanation,
            "question": question,
            "answer": answer
        }
        return self._run_llm_chain(prompt, inputs)
    
    def evaluate_explanation_quality(self, explanation: str, question: str, answer: str) -> Dict[str, float]:
        """èª¬æ˜ã®å“è³ªã‚’è©•ä¾¡"""
    
        metrics = {
            'length': len(explanation),
            'readability_score': self._calculate_readability(explanation),
            'structure_score': self._evaluate_structure(explanation),
            'completeness_score': self._evaluate_completeness(explanation, question, answer),
            'timestamp': datetime.now().isoformat()
        }
        
        # ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        metrics['overall_score'] = np.mean([
            metrics['readability_score'],
            metrics['structure_score'],
            metrics['completeness_score']
        ])
        
        return metrics

    def _calculate_readability(self, text: str) -> float:
        """å¯èª­æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-1ï¼‰"""
        # ç°¡æ˜“çš„ãªå®Ÿè£…
        avg_sentence_length = np.mean([len(s.split()) for s in text.split('ã€‚') if s])
        
        # ç†æƒ³çš„ãªæ–‡é•·ã¯15-20å˜èª
        if 15 <= avg_sentence_length <= 20:
            return 1.0
        elif avg_sentence_length < 10 or avg_sentence_length > 30:
            return 0.5
        else:
            return 0.8

    def _evaluate_structure(self, text: str) -> float:
        """æ§‹é€ ã®è‰¯ã•ã‚’è©•ä¾¡ï¼ˆ0-1ï¼‰"""
        structure_elements = ['##', '###', '1.', '2.', '3.', '-', '*', '|']
        found_elements = sum(1 for elem in structure_elements if elem in text)
        
        # æ§‹é€ è¦ç´ ãŒå¤šã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
        return min(found_elements / 5.0, 1.0)

    def _evaluate_completeness(self, explanation: str, question: str, answer: str) -> float:
        """èª¬æ˜ã®å®Œå…¨æ€§ã‚’è©•ä¾¡ï¼ˆ0-1ï¼‰"""
        # è³ªå•ã¨å›ç­”ã®ä¸»è¦ãªè¦ç´ ãŒèª¬æ˜ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        question_words = set(question.split())
        answer_words = set(answer.split())
        explanation_words = set(explanation.split())
        
        question_coverage = len(question_words & explanation_words) / len(question_words)
        answer_coverage = len(answer_words & explanation_words) / len(answer_words)
        
        return (question_coverage + answer_coverage) / 2.0

